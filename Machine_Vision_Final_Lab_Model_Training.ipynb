{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/jvonrad/Machine-Vision-PushUp-Detector/blob/main/Machine_Vision_Final_Lab_Model_Training.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "d4XrZuAWeoUc",
        "outputId": "90bf1f2e-9ace-40bc-99ac-2f63466c8ed0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/140.6 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m140.6/140.6 kB\u001b[0m \u001b[31m12.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/14.6 MB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━━━━━━\u001b[0m \u001b[32m10.0/14.6 MB\u001b[0m \u001b[31m300.2 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m14.6/14.6 MB\u001b[0m \u001b[31m295.0 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m14.6/14.6 MB\u001b[0m \u001b[31m148.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/86.8 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m86.8/86.8 kB\u001b[0m \u001b[31m10.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: opencv-python in /usr/local/lib/python3.12/dist-packages (4.12.0.88)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.12/dist-packages (2.9.0+cu126)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.12/dist-packages (2.0.2)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.12/dist-packages (0.24.0+cu126)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from torch) (3.20.2)\n",
            "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.12/dist-packages (from torch) (4.15.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from torch) (75.2.0)\n",
            "Requirement already satisfied: sympy>=1.13.3 in /usr/local/lib/python3.12/dist-packages (from torch) (1.14.0)\n",
            "Requirement already satisfied: networkx>=2.5.1 in /usr/local/lib/python3.12/dist-packages (from torch) (3.6.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from torch) (3.1.6)\n",
            "Requirement already satisfied: fsspec>=0.8.5 in /usr/local/lib/python3.12/dist-packages (from torch) (2025.3.0)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.6.80 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.80)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.10.2.21 in /usr/local/lib/python3.12/dist-packages (from torch) (9.10.2.21)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.6.4.1 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.4.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.3.0.4 in /usr/local/lib/python3.12/dist-packages (from torch) (11.3.0.4)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.7.77 in /usr/local/lib/python3.12/dist-packages (from torch) (10.3.7.77)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.7.1.2 in /usr/local/lib/python3.12/dist-packages (from torch) (11.7.1.2)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.5.4.2 in /usr/local/lib/python3.12/dist-packages (from torch) (12.5.4.2)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.7.1 in /usr/local/lib/python3.12/dist-packages (from torch) (0.7.1)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.27.5 in /usr/local/lib/python3.12/dist-packages (from torch) (2.27.5)\n",
            "Requirement already satisfied: nvidia-nvshmem-cu12==3.3.20 in /usr/local/lib/python3.12/dist-packages (from torch) (3.3.20)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.77)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.6.85 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.85)\n",
            "Requirement already satisfied: nvidia-cufile-cu12==1.11.1.6 in /usr/local/lib/python3.12/dist-packages (from torch) (1.11.1.6)\n",
            "Requirement already satisfied: triton==3.5.0 in /usr/local/lib/python3.12/dist-packages (from torch) (3.5.0)\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.12/dist-packages (from torchvision) (11.3.0)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy>=1.13.3->torch) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->torch) (3.0.3)\n",
            "Requirement already satisfied: opencv-python in /usr/local/lib/python3.12/dist-packages (4.12.0.88)\n",
            "Requirement already satisfied: numpy<2.3.0,>=2 in /usr/local/lib/python3.12/dist-packages (from opencv-python) (2.0.2)\n",
            "Collecting mediapipe==0.10.13\n",
            "  Downloading mediapipe-0.10.13-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (9.7 kB)\n",
            "Requirement already satisfied: absl-py in /usr/local/lib/python3.12/dist-packages (from mediapipe==0.10.13) (1.4.0)\n",
            "Requirement already satisfied: attrs>=19.1.0 in /usr/local/lib/python3.12/dist-packages (from mediapipe==0.10.13) (25.4.0)\n",
            "Requirement already satisfied: flatbuffers>=2.0 in /usr/local/lib/python3.12/dist-packages (from mediapipe==0.10.13) (25.12.19)\n",
            "Requirement already satisfied: jax in /usr/local/lib/python3.12/dist-packages (from mediapipe==0.10.13) (0.7.2)\n",
            "Requirement already satisfied: jaxlib in /usr/local/lib/python3.12/dist-packages (from mediapipe==0.10.13) (0.7.2)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.12/dist-packages (from mediapipe==0.10.13) (3.10.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.12/dist-packages (from mediapipe==0.10.13) (2.0.2)\n",
            "Requirement already satisfied: opencv-contrib-python in /usr/local/lib/python3.12/dist-packages (from mediapipe==0.10.13) (4.12.0.88)\n",
            "Collecting protobuf<5,>=4.25.3 (from mediapipe==0.10.13)\n",
            "  Downloading protobuf-4.25.8-cp37-abi3-manylinux2014_x86_64.whl.metadata (541 bytes)\n",
            "Collecting sounddevice>=0.4.4 (from mediapipe==0.10.13)\n",
            "  Downloading sounddevice-0.5.3-py3-none-any.whl.metadata (1.6 kB)\n",
            "Requirement already satisfied: CFFI>=1.0 in /usr/local/lib/python3.12/dist-packages (from sounddevice>=0.4.4->mediapipe==0.10.13) (2.0.0)\n",
            "Requirement already satisfied: ml_dtypes>=0.5.0 in /usr/local/lib/python3.12/dist-packages (from jax->mediapipe==0.10.13) (0.5.4)\n",
            "Requirement already satisfied: opt_einsum in /usr/local/lib/python3.12/dist-packages (from jax->mediapipe==0.10.13) (3.4.0)\n",
            "Requirement already satisfied: scipy>=1.13 in /usr/local/lib/python3.12/dist-packages (from jax->mediapipe==0.10.13) (1.16.3)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib->mediapipe==0.10.13) (1.3.3)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.12/dist-packages (from matplotlib->mediapipe==0.10.13) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.12/dist-packages (from matplotlib->mediapipe==0.10.13) (4.61.1)\n",
            "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib->mediapipe==0.10.13) (1.4.9)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.12/dist-packages (from matplotlib->mediapipe==0.10.13) (25.0)\n",
            "Requirement already satisfied: pillow>=8 in /usr/local/lib/python3.12/dist-packages (from matplotlib->mediapipe==0.10.13) (11.3.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib->mediapipe==0.10.13) (3.3.1)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.12/dist-packages (from matplotlib->mediapipe==0.10.13) (2.9.0.post0)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.12/dist-packages (from CFFI>=1.0->sounddevice>=0.4.4->mediapipe==0.10.13) (2.23)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.12/dist-packages (from python-dateutil>=2.7->matplotlib->mediapipe==0.10.13) (1.17.0)\n",
            "Downloading mediapipe-0.10.13-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (35.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m35.6/35.6 MB\u001b[0m \u001b[31m52.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading protobuf-4.25.8-cp37-abi3-manylinux2014_x86_64.whl (294 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m294.9/294.9 kB\u001b[0m \u001b[31m33.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading sounddevice-0.5.3-py3-none-any.whl (32 kB)\n",
            "Installing collected packages: protobuf, sounddevice, mediapipe\n",
            "  Attempting uninstall: protobuf\n",
            "    Found existing installation: protobuf 5.29.5\n",
            "    Uninstalling protobuf-5.29.5:\n",
            "      Successfully uninstalled protobuf-5.29.5\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "ydf 0.13.0 requires protobuf<7.0.0,>=5.29.1, but you have protobuf 4.25.8 which is incompatible.\n",
            "grpcio-status 1.71.2 requires protobuf<6.0dev,>=5.26.1, but you have protobuf 4.25.8 which is incompatible.\n",
            "opentelemetry-proto 1.37.0 requires protobuf<7.0,>=5.0, but you have protobuf 4.25.8 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed mediapipe-0.10.13 protobuf-4.25.8 sounddevice-0.5.3\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "google"
                ]
              },
              "id": "797d0597d8904dc18c737c4ee1b7c18c"
            }
          },
          "metadata": {}
        }
      ],
      "source": [
        "!pip install boto3 -q\n",
        "!pip install opencv-python torch numpy torchvision\n",
        "!pip install opencv-python\n",
        "!pip install 'mediapipe==0.10.13'\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PMr99Yo7x8N1"
      },
      "source": [
        "## Download the data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4YhoE1nF2Pee"
      },
      "source": [
        "The data for this assignment has been made available and is downloadable to disk by running the below cell."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "B5ekP01hR9VV",
        "outputId": "caaa16da-dce0-43e8-ccce-2a091f528534"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading: 1_dksksjfwijf.mp4\n",
            "Downloading: 2_dfsaeklnvvalkej.mp4\n",
            "Downloading: 2_difficult_2.mp4\n",
            "Downloading: 2_difficult_sdafkljsalkfj.mp4\n",
            "Downloading: 2_dkdjwkndkfw.mp4\n",
            "Downloading: 2_dkdmkejkeimdh.mp4\n",
            "Downloading: 2_dkjd823kjf.mp4\n",
            "Downloading: 2_dsalkfjalwkenlke.mp4\n",
            "Downloading: 2_kling_20251205_Text_to_Video_On_a_sandy_4976_0.mp4\n",
            "Downloading: 2_kling_20251206_Text_to_Video_Generate_a_71_1.mp4\n",
            "Downloading: 2_sadfasjldkfjaseifj.mp4\n",
            "Downloading: 2_sdafkjaslkclaksdjkas.mp4\n",
            "Downloading: 2_sdfkjsaleijflaskdjf.mp4\n",
            "Downloading: 2_sdjfhafsldkjhjk.mp4\n",
            "Downloading: 2_sdkjdsflkjfwa.mp4\n",
            "Downloading: 2_sdlfjlewlkjkj.mp4\n",
            "Downloading: 2_sdlkjsaelijfksdjf.mp4\n",
            "Downloading: 3_asldkfjalwieaskdfaskdf.mp4\n",
            "Downloading: 3_dkk873lkjlksajdf.mp4\n",
            "Downloading: 3_dsjlaeijlksjdfie.mp4\n",
            "Downloading: 3_dsksdfjbvsdkj.mp4\n",
            "Downloading: 3_dslkaldskjflakjs.mp4\n",
            "Downloading: 3_ewdfkjwaeoihjlkasdjf.mp4\n",
            "Downloading: 3_kling_20251205_Text_to_Video_In_a_grass_4697_0.mp4\n",
            "Downloading: 3_kling_20251205_Text_to_Video_On_a_playg_5028_0.mp4\n",
            "Downloading: 3_kling_20251205_Text_to_Video_On_a_playg_5064_0.mp4\n",
            "Downloading: 3_kling_20251206_Text_to_Video_Generate_a_17_0.mp4\n",
            "Downloading: 3_kling_20251206_Text_to_Video_Generate_a_315_0.mp4\n",
            "Downloading: 3_kling_20251206_Text_to_Video_Generate_a_315_2.mp4\n",
            "Downloading: 3_kling_20251206_Text_to_Video_Generate_a_712_3.mp4\n",
            "Downloading: 3_kling_20251206_Text_to_Video_Generate_a_71_0.mp4\n",
            "Downloading: 3_kling_20251206_Text_to_Video_Generate_a_71_2.mp4\n",
            "Downloading: 3_kling_20251206_Text_to_Video_Generate_a_71_3.mp4\n",
            "Downloading: 3_kling_20251209_Image_to_Video_Generate_a_613_1.mp4\n",
            "Downloading: 3_kling_20251209_Image_to_Video_Generate_a_635_0.mp4\n",
            "Downloading: 3_kling_20251209_Text_to_Video_Generate_a_190_1.mp4\n",
            "Downloading: 3_kling_20251209_Text_to_Video_Generate_a_403_1.mp4\n",
            "Downloading: 3_kling_20251209_Text_to_Video_Generate_a_491_0.mp4\n",
            "Downloading: 3_kling_20251209_Text_to_Video_Generate_a_491_1.mp4\n",
            "Downloading: 3_kling_20251209_Text_to_Video_Generate_a_491_2.mp4\n",
            "Downloading: 3_kling_dskfseu.mp4\n",
            "Downloading: 3_kling_kdjflaskdjf.mp4\n",
            "Downloading: 3_sadklfjasbnlkjlfkj.mp4\n",
            "Downloading: 3_sadlfkjasldkfjasleijlkjfd.mp4\n",
            "Downloading: 3_sadlfkjawelnflksdjf.mp4\n",
            "Downloading: 3_sdfjwaiejflkasjdf.mp4\n",
            "Downloading: 3_sdflkjliejkjdf.mp4\n",
            "Downloading: 3_sdlkfjaleknaksej.mp4\n",
            "Downloading: 3_sdlkfjalkjejafe.mp4\n",
            "Downloading: 3_sdlkjfaslkjfalskjdf.mp4\n",
            "Downloading: 3_sdlkjslndflkseijlkjef.mp4\n",
            "Downloading: 4_20251209_Text_to_Video_Generate_a_561_0.mp4\n",
            "Downloading: 4_asdlkfjalsflnekj.mp4\n",
            "Downloading: 4_aslkcasckmwlejk.mp4\n",
            "Downloading: 4_aslkjasmcalkewjlkje.mp4\n",
            "Downloading: 4_dssalsdkfjweijf.mp4\n",
            "Downloading: 4_kling_20251206_Text_to_Video_Generate_a_28_0.mp4\n",
            "Downloading: 4_kling_20251206_Text_to_Video_Generate_a_315_3.mp4\n",
            "Downloading: 4_kling_20251206_Text_to_Video_Generate_a_58_0.mp4\n",
            "Downloading: 4_kling_20251207_Text_to_Video_Generate_a_521_1.mp4\n",
            "Downloading: 4_kling_20251209_Image_to_Video_Generate_a_635_1.mp4\n",
            "Downloading: 4_kling_20251209_Text_to_Video_Generate_a_190_0.mp4\n",
            "Downloading: 4_kling_20251209_Text_to_Video_Generate_a_218_0.mp4\n",
            "Downloading: 4_kling_20251209_Text_to_Video_Generate_a_263_1.mp4\n",
            "Downloading: 4_kling_20251209_Text_to_Video_Generate_a_377_1.mp4\n",
            "Downloading: 4_kling_20251209_Text_to_Video_Generate_a_452_0.mp4\n",
            "Downloading: 4_kling_20251209_Text_to_Video_Generate_a_452_1.mp4\n",
            "Downloading: 4_kling_20251209_Text_to_Video_Generate_a_561_1.mp4\n",
            "Downloading: 4_kling_20251209_Text_to_Video_Generate_a_588_2.mp4\n",
            "Downloading: 4_pushup_1f2da596-7619-4d55-9376-069e15a42a1a_h264.mp4\n",
            "Downloading: 4_sadflkjasldkjfalseij.mp4\n",
            "Downloading: 4_sadlfkjlknewkjejk.mp4\n",
            "Downloading: 5_sadfjhaslfkjasdlkfjsa.mp4\n",
            "Downloading: 5_sdfkljweoijlkjdsflkjweaij.mp4\n",
            "Downloading: 6_dfjewaijsldkjfsaef.mp4\n",
            "Downloading: 6_kling_20251209_Text_to_Video_Generate_a_218_1.mp4\n",
            "Downloading: 7_sadkjfkljekj.mp4\n",
            "\n",
            "==================================================\n",
            "Downloaded videos:\n",
            "==================================================\n",
            "1_dksksjfwijf.mp4\n",
            "2_dfsaeklnvvalkej.mp4\n",
            "2_difficult_2.mp4\n",
            "2_difficult_sdafkljsalkfj.mp4\n",
            "2_dkdjwkndkfw.mp4\n",
            "2_dkdmkejkeimdh.mp4\n",
            "2_dkjd823kjf.mp4\n",
            "2_dsalkfjalwkenlke.mp4\n",
            "2_kling_20251205_Text_to_Video_On_a_sandy_4976_0.mp4\n",
            "2_kling_20251206_Text_to_Video_Generate_a_71_1.mp4\n",
            "2_sadfasjldkfjaseifj.mp4\n",
            "2_sdafkjaslkclaksdjkas.mp4\n",
            "2_sdfkjsaleijflaskdjf.mp4\n",
            "2_sdjfhafsldkjhjk.mp4\n",
            "2_sdkjdsflkjfwa.mp4\n",
            "2_sdlfjlewlkjkj.mp4\n",
            "2_sdlkjsaelijfksdjf.mp4\n",
            "3_asldkfjalwieaskdfaskdf.mp4\n",
            "3_dkk873lkjlksajdf.mp4\n",
            "3_dsjlaeijlksjdfie.mp4\n",
            "3_dsksdfjbvsdkj.mp4\n",
            "3_dslkaldskjflakjs.mp4\n",
            "3_ewdfkjwaeoihjlkasdjf.mp4\n",
            "3_kling_20251205_Text_to_Video_In_a_grass_4697_0.mp4\n",
            "3_kling_20251205_Text_to_Video_On_a_playg_5028_0.mp4\n",
            "3_kling_20251205_Text_to_Video_On_a_playg_5064_0.mp4\n",
            "3_kling_20251206_Text_to_Video_Generate_a_17_0.mp4\n",
            "3_kling_20251206_Text_to_Video_Generate_a_315_0.mp4\n",
            "3_kling_20251206_Text_to_Video_Generate_a_315_2.mp4\n",
            "3_kling_20251206_Text_to_Video_Generate_a_712_3.mp4\n",
            "3_kling_20251206_Text_to_Video_Generate_a_71_0.mp4\n",
            "3_kling_20251206_Text_to_Video_Generate_a_71_2.mp4\n",
            "3_kling_20251206_Text_to_Video_Generate_a_71_3.mp4\n",
            "3_kling_20251209_Image_to_Video_Generate_a_613_1.mp4\n",
            "3_kling_20251209_Image_to_Video_Generate_a_635_0.mp4\n",
            "3_kling_20251209_Text_to_Video_Generate_a_190_1.mp4\n",
            "3_kling_20251209_Text_to_Video_Generate_a_403_1.mp4\n",
            "3_kling_20251209_Text_to_Video_Generate_a_491_0.mp4\n",
            "3_kling_20251209_Text_to_Video_Generate_a_491_1.mp4\n",
            "3_kling_20251209_Text_to_Video_Generate_a_491_2.mp4\n",
            "3_kling_dskfseu.mp4\n",
            "3_kling_kdjflaskdjf.mp4\n",
            "3_sadklfjasbnlkjlfkj.mp4\n",
            "3_sadlfkjasldkfjasleijlkjfd.mp4\n",
            "3_sadlfkjawelnflksdjf.mp4\n",
            "3_sdfjwaiejflkasjdf.mp4\n",
            "3_sdflkjliejkjdf.mp4\n",
            "3_sdlkfjaleknaksej.mp4\n",
            "3_sdlkfjalkjejafe.mp4\n",
            "3_sdlkjfaslkjfalskjdf.mp4\n",
            "3_sdlkjslndflkseijlkjef.mp4\n",
            "4_20251209_Text_to_Video_Generate_a_561_0.mp4\n",
            "4_asdlkfjalsflnekj.mp4\n",
            "4_aslkcasckmwlejk.mp4\n",
            "4_aslkjasmcalkewjlkje.mp4\n",
            "4_dssalsdkfjweijf.mp4\n",
            "4_kling_20251206_Text_to_Video_Generate_a_28_0.mp4\n",
            "4_kling_20251206_Text_to_Video_Generate_a_315_3.mp4\n",
            "4_kling_20251206_Text_to_Video_Generate_a_58_0.mp4\n",
            "4_kling_20251207_Text_to_Video_Generate_a_521_1.mp4\n",
            "4_kling_20251209_Image_to_Video_Generate_a_635_1.mp4\n",
            "4_kling_20251209_Text_to_Video_Generate_a_190_0.mp4\n",
            "4_kling_20251209_Text_to_Video_Generate_a_218_0.mp4\n",
            "4_kling_20251209_Text_to_Video_Generate_a_263_1.mp4\n",
            "4_kling_20251209_Text_to_Video_Generate_a_377_1.mp4\n",
            "4_kling_20251209_Text_to_Video_Generate_a_452_0.mp4\n",
            "4_kling_20251209_Text_to_Video_Generate_a_452_1.mp4\n",
            "4_kling_20251209_Text_to_Video_Generate_a_561_1.mp4\n",
            "4_kling_20251209_Text_to_Video_Generate_a_588_2.mp4\n",
            "4_pushup_1f2da596-7619-4d55-9376-069e15a42a1a_h264.mp4\n",
            "4_sadflkjasldkjfalseij.mp4\n",
            "4_sadlfkjlknewkjejk.mp4\n",
            "5_sadfjhaslfkjasdlkfjsa.mp4\n",
            "5_sdfkljweoijlkjdsflkjweaij.mp4\n",
            "6_dfjewaijsldkjfsaef.mp4\n",
            "6_kling_20251209_Text_to_Video_Generate_a_218_1.mp4\n",
            "7_sadkjfkljekj.mp4\n",
            "\n",
            "Total: 77 files\n"
          ]
        }
      ],
      "source": [
        "import re\n",
        "import boto3\n",
        "from botocore import UNSIGNED\n",
        "from botocore.config import Config\n",
        "import os\n",
        "import cv2\n",
        "\n",
        "\n",
        "def download_videos():\n",
        "    # Connect to S3 without authentication (public bucket)\n",
        "    s3 = boto3.client('s3', config=Config(signature_version=UNSIGNED))\n",
        "\n",
        "    bucket_name = 'prism-mvta'\n",
        "    prefix = 'training-and-validation-data/'\n",
        "    download_dir = './video-data'\n",
        "\n",
        "    os.makedirs(download_dir, exist_ok=True)\n",
        "\n",
        "    # List all objects in the S3 path\n",
        "    paginator = s3.get_paginator('list_objects_v2')\n",
        "    pages = paginator.paginate(Bucket=bucket_name, Prefix=prefix)\n",
        "\n",
        "    video_names = []\n",
        "\n",
        "    for page in pages:\n",
        "        if 'Contents' not in page:\n",
        "            print(\"No files found at the specified path! Go and complain to the TAs!\")\n",
        "            break\n",
        "\n",
        "        for obj in page['Contents']:\n",
        "            key = obj['Key']\n",
        "            filename = os.path.basename(key)\n",
        "\n",
        "            if not filename:\n",
        "                continue\n",
        "\n",
        "            video_names.append(filename)\n",
        "\n",
        "            local_path = os.path.join(download_dir, filename)\n",
        "            print(f\"Downloading: {filename}\")\n",
        "            s3.download_file(bucket_name, key, local_path)\n",
        "\n",
        "    print(\"\\n\" + \"=\"*50)\n",
        "    print(\"Downloaded videos:\")\n",
        "    print(\"=\"*50)\n",
        "    for name in video_names:\n",
        "        print(name)\n",
        "\n",
        "    print(f\"\\nTotal: {len(video_names)} files\")\n",
        "\n",
        "download_videos()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ep_hJ9Czox3V"
      },
      "source": [
        "## MediaPipe Pose Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "j737dP-Jox3V"
      },
      "outputs": [],
      "source": [
        "import mediapipe as mp\n",
        "\n",
        "class MediaPipePoseExtractor:\n",
        "    def __init__(self, static_image_mode=False, model_complexity=1, smooth_landmarks=True):\n",
        "        self.mp_pose = mp.solutions.pose\n",
        "        self.pose = self.mp_pose.Pose(\n",
        "            static_image_mode=static_image_mode,\n",
        "            model_complexity=model_complexity,\n",
        "            smooth_landmarks=smooth_landmarks,\n",
        "            enable_segmentation=False,\n",
        "            min_detection_confidence=0.5,\n",
        "            min_tracking_confidence=0.5\n",
        "        )\n",
        "\n",
        "    def extract(self, video_path, max_frames=None, stride=1, include_visibility=True):\n",
        "        cap = cv2.VideoCapture(video_path)\n",
        "        keypoints = []\n",
        "        frame_idx = 0\n",
        "\n",
        "        while True:\n",
        "            ret, frame = cap.read()\n",
        "            if not ret:\n",
        "                break\n",
        "\n",
        "            if frame_idx % stride != 0:\n",
        "                frame_idx += 1\n",
        "                continue\n",
        "\n",
        "            rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
        "            results = self.pose.process(rgb)\n",
        "\n",
        "            if results.pose_landmarks is None:\n",
        "                # if detection fails, fill zeros\n",
        "                if include_visibility:\n",
        "                    kp = np.zeros((33, 3), dtype=np.float32)\n",
        "                else:\n",
        "                    kp = np.zeros((33, 2), dtype=np.float32)\n",
        "            else:\n",
        "                lm = results.pose_landmarks.landmark\n",
        "                if include_visibility:\n",
        "                    kp = np.array([[p.x, p.y, p.visibility]\n",
        "                                  for p in lm], dtype=np.float32)\n",
        "                else:\n",
        "                    kp = np.array([[p.x, p.y] for p in lm], dtype=np.float32)\n",
        "\n",
        "            keypoints.append(kp)\n",
        "\n",
        "            frame_idx += 1\n",
        "            if max_frames is not None and len(keypoints) >= max_frames:\n",
        "                break\n",
        "\n",
        "        cap.release()\n",
        "        return np.stack(keypoints, axis=0)  # (T, 33, D)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5wPAlvHdXj3a"
      },
      "source": [
        "These videos are now available in the folder \"video-data\". You can click on the folder icon on the left-hand-side of this screen to see the videos in a file explorer."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vtisgbeiYiH_"
      },
      "source": [
        "# Create your Datasets and Dataloaders"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Xo9J9hXLeCdY"
      },
      "source": [
        "Some example code for approaching the first *two* TODOs is given below just to get you started. No starter code is given for the third TODO.\n",
        "\n",
        "Note, the below code is very rough skeleton code. Make no assumptions as to the correct manner to architect your model based on the structure of this code.\n",
        "\n",
        "Please feel free to (if not encouraged to) change every single line of the below code (change it to best suit your chosen model architecture, in the next section)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XzDMxGLnYa0s"
      },
      "source": [
        "### TODO 1 (This is mostly already done for you - Please see the v1 provided below)\n",
        "\n",
        "Each video in the folder is prefixed by a number. That number corresponds to the number of distinct pushups visible in the video. Write code to iterate over each video in the folder, and extract the corresponding target associated with the video."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "74PvwbsYYMlD"
      },
      "source": [
        "### TODO 2 (This is also mostly already done for you - Please see the v1 provided below)\n",
        "\n",
        "\n",
        "Divide the data into training and validation sets.\n",
        "\n",
        "Optionally, you can also create out your own test set to assess your performance."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hEaV_5oXZQRc"
      },
      "source": [
        "### TODO 3\n",
        "\n",
        "Any preprocessing or augmentation of your data which you deem required, should (probably) go here. You are also free to include your data-augmentation code later, though doing it before creating your dataloaders is probably a good idea.\n",
        "\n",
        "If you complete this TODO, to maintain experimental hygiene, feel free to modify the code which was provided for TODOs 1 and 2."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "TvqxH1YBYCUw"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import Dataset, DataLoader, random_split\n",
        "import os\n",
        "import cv2\n",
        "import numpy as np\n",
        "\n",
        "\n",
        "class VideoDataset(Dataset):\n",
        "    \"\"\"Dataset for loading videos from a folder. Labels from filename prefix.\"\"\"\n",
        "\n",
        "    def __init__(self, video_dir, frame_size=(224, 224), transform=None):\n",
        "        self.video_dir = video_dir\n",
        "        self.frame_size = frame_size\n",
        "        self.transform = transform\n",
        "\n",
        "        self.video_files = [\n",
        "            f for f in os.listdir(video_dir)\n",
        "            if f.endswith(('.mp4', '.avi', '.mov'))\n",
        "        ]\n",
        "\n",
        "        self.labels = [\n",
        "            int(f.split('_')[0]) for f in self.video_files\n",
        "        ]\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.video_files)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        video_path = os.path.join(self.video_dir, self.video_files[idx])\n",
        "        frames = self._load_video(video_path)\n",
        "        label = self.labels[idx]\n",
        "\n",
        "        if self.transform:\n",
        "            frames = self.transform(frames)\n",
        "\n",
        "        return frames, label\n",
        "\n",
        "    def _load_video(self, path):\n",
        "        cap = cv2.VideoCapture(path)\n",
        "\n",
        "        frames = []\n",
        "        while True:\n",
        "            ret, frame = cap.read()\n",
        "            if not ret:\n",
        "                break\n",
        "            frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
        "            frame = cv2.resize(frame, (self.frame_size[1], self.frame_size[0]))\n",
        "            frames.append(frame)\n",
        "\n",
        "        cap.release()\n",
        "\n",
        "        frames = torch.from_numpy(np.array(frames)).permute(3, 0, 1, 2).float() / 255.0\n",
        "\n",
        "        return frames\n",
        "\n",
        "\n",
        "import os\n",
        "import cv2\n",
        "import numpy as np\n",
        "from pathlib import Path\n",
        "import random\n",
        "\n",
        "def augment_and_save_video(\n",
        "    in_path: str,\n",
        "    out_path: str,\n",
        "    crop_scale_range=(0.80, 1.00),\n",
        "    flip_prob=0.5,\n",
        "    rot_range=(-10, 10),        # degrees\n",
        "    scale_range=(0.9, 1.1),     # scaling factor\n",
        "    trans_frac=0.1,          # fraction of width/height\n",
        "    seed=None\n",
        "):\n",
        "    if seed is not None:\n",
        "        random.seed(seed)\n",
        "\n",
        "    cap = cv2.VideoCapture(in_path)\n",
        "    if not cap.isOpened():\n",
        "        raise RuntimeError(f\"Could not open video: {in_path}\")\n",
        "\n",
        "    fps = cap.get(cv2.CAP_PROP_FPS)\n",
        "    w   = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
        "    h   = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
        "\n",
        "    # force output extension to .avi\n",
        "    if out_path.lower().endswith(\".mp4\"):\n",
        "        out_path = out_path[:-4] + \".avi\"\n",
        "\n",
        "    fourcc = cv2.VideoWriter_fourcc(*\"MJPG\")\n",
        "    out = cv2.VideoWriter(out_path, fourcc, fps, (w, h))\n",
        "\n",
        "    if not out.isOpened():\n",
        "        cap.release()\n",
        "        raise RuntimeError(f\"Could not open VideoWriter: {out_path}\")\n",
        "\n",
        "    # ---------- Sample augmentation parameters ONCE per video ----------\n",
        "    do_flip = (random.random() < flip_prob)\n",
        "\n",
        "    # cropping\n",
        "    scale_crop = random.uniform(*crop_scale_range)\n",
        "    crop_w = int(w * scale_crop)\n",
        "    crop_h = int(h * scale_crop)\n",
        "    x0 = 0 if crop_w == w else random.randint(0, w - crop_w)\n",
        "    y0 = 0 if crop_h == h else random.randint(0, h - crop_h)\n",
        "\n",
        "    # camera geometry jitter\n",
        "    rot_deg = random.uniform(*rot_range)\n",
        "    scale   = random.uniform(*scale_range)\n",
        "    tx      = random.uniform(-trans_frac, trans_frac) * w\n",
        "    ty      = random.uniform(-trans_frac, trans_frac) * h\n",
        "\n",
        "    M = cv2.getRotationMatrix2D((w / 2, h / 2), rot_deg, scale)\n",
        "    M[:, 2] += (tx, ty)\n",
        "\n",
        "    # --------------------------------------------------------------\n",
        "\n",
        "    while True:\n",
        "        ret, frame = cap.read()\n",
        "        if not ret:\n",
        "            break\n",
        "\n",
        "        # 1) camera geometry warp (simulate camera movement)\n",
        "        frame = cv2.warpAffine(\n",
        "            frame, M, (w, h),\n",
        "            flags=cv2.INTER_LINEAR,\n",
        "            borderMode=cv2.BORDER_REFLECT101\n",
        "        )\n",
        "\n",
        "        # 2) crop\n",
        "        cropped = frame[y0:y0 + crop_h, x0:x0 + crop_w]\n",
        "\n",
        "        # 3) resize back to original size\n",
        "        resized = cv2.resize(cropped, (w, h), interpolation=cv2.INTER_LINEAR)\n",
        "\n",
        "        # 4) horizontal flip\n",
        "        if do_flip:\n",
        "            resized = cv2.flip(resized, 1)\n",
        "\n",
        "        out.write(resized)\n",
        "\n",
        "    cap.release()\n",
        "    out.release()\n",
        "\n",
        "\n",
        "def build_augmented_dataset(\n",
        "    src_dir=\"./video-data\",\n",
        "    dst_dir=\"./video-data-aug\",\n",
        "    copies_per_video=2,\n",
        "    crop_scale_range=(0.80, 1.00),\n",
        "    flip_prob=0.5,\n",
        "    seed=42\n",
        "):\n",
        "    src_dir = Path(src_dir)\n",
        "    dst_dir = Path(dst_dir)\n",
        "    dst_dir.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "    rng = random.Random(seed)\n",
        "\n",
        "    video_files = sorted([p for p in src_dir.iterdir() if p.suffix.lower() == \".mp4\"])\n",
        "    print(f\"Found {len(video_files)} original videos\")\n",
        "\n",
        "    # 1) copy originals into dst_dir (so dst_dir becomes \"expanded set\")\n",
        "    for vid in video_files:\n",
        "        out_orig = dst_dir / vid.name\n",
        "        if not out_orig.exists():\n",
        "            out_orig.write_bytes(vid.read_bytes())\n",
        "\n",
        "    # 2) create augmented copies\n",
        "    for vid in video_files:\n",
        "        stem = vid.stem  # e.g. \"4_pushup_....\"\n",
        "        for k in range(copies_per_video):\n",
        "            out_name = f\"{stem}__aug{k}.mp4\"\n",
        "            out_path = dst_dir / out_name\n",
        "\n",
        "            if out_path.exists():\n",
        "                continue\n",
        "\n",
        "            # different seed per output\n",
        "            aug_seed = rng.randint(0, 10**9)\n",
        "            augment_and_save_video(\n",
        "                str(vid),\n",
        "                str(out_path),\n",
        "                crop_scale_range=crop_scale_range,\n",
        "                flip_prob=flip_prob,\n",
        "                seed=aug_seed\n",
        "            )\n",
        "\n",
        "        print(f\"Augmented: {vid.name}\")\n",
        "\n",
        "    print(f\"Expanded dataset written to: {dst_dir} (originals + aug copies)\")\n",
        "    return str(dst_dir)\n",
        "\n",
        "import os\n",
        "import random\n",
        "from pathlib import Path\n",
        "from collections import defaultdict\n",
        "\n",
        "def parse_label_from_filename(fname: str) -> int:\n",
        "    # \"4_pushup_abc.mp4\" -> 4\n",
        "    return int(fname.split(\"_\")[0])\n",
        "\n",
        "def build_augmented_dataset_balanced(\n",
        "    src_dir=\"./video-data\",\n",
        "    dst_dir=\"./video-data-aug\",\n",
        "    target_per_class=50,\n",
        "    crop_scale_range=(0.80, 1.00),\n",
        "    flip_prob=0.5,\n",
        "    rot_range=(-10, 10),\n",
        "    scale_range=(0.9, 1.1),\n",
        "    trans_frac=0.1,\n",
        "    seed=42,\n",
        "    copy_originals=True,\n",
        "):\n",
        "    src_dir = Path(src_dir)\n",
        "    dst_dir = Path(dst_dir)\n",
        "    dst_dir.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "    rng = random.Random(seed)\n",
        "\n",
        "    # --- gather originals by class ---\n",
        "    originals_by_label = defaultdict(list)\n",
        "    for p in sorted(src_dir.iterdir()):\n",
        "        if p.suffix.lower() not in [\".mp4\", \".avi\"]:\n",
        "            continue\n",
        "        try:\n",
        "            y = parse_label_from_filename(p.name)\n",
        "        except Exception:\n",
        "            continue\n",
        "        originals_by_label[y].append(p)\n",
        "\n",
        "    if not originals_by_label:\n",
        "        raise RuntimeError(f\"No .mp4 videos found in {src_dir}\")\n",
        "\n",
        "    print(\"Original counts per class:\")\n",
        "    for y in sorted(originals_by_label):\n",
        "        print(f\"  {y}: {len(originals_by_label[y])}\")\n",
        "\n",
        "    # --- optionally copy originals into dst ---\n",
        "    if copy_originals:\n",
        "        for y, vids in originals_by_label.items():\n",
        "            for vid in vids:\n",
        "                out_orig = dst_dir / vid.name\n",
        "                if not out_orig.exists():\n",
        "                    out_orig.write_bytes(vid.read_bytes())\n",
        "\n",
        "    # --- find how many already exist in dst_dir (originals + prior augs) ---\n",
        "    existing_by_label = defaultdict(int)\n",
        "    for p in dst_dir.iterdir():\n",
        "        if p.suffix.lower() not in [\".mp4\", \".avi\"]:\n",
        "            continue\n",
        "        try:\n",
        "            y = parse_label_from_filename(p.name)\n",
        "        except Exception:\n",
        "            continue\n",
        "        existing_by_label[y] += 1\n",
        "\n",
        "    print(\"\\nExisting counts in dst_dir before new augs:\")\n",
        "    for y in sorted(originals_by_label):\n",
        "        print(f\"  {y}: {existing_by_label[y]}\")\n",
        "\n",
        "    # --- generate more until each class hits target_per_class ---\n",
        "    for y in sorted(originals_by_label):\n",
        "        have = existing_by_label[y]\n",
        "        need = max(0, target_per_class - have)\n",
        "\n",
        "        if need == 0:\n",
        "            print(f\"\\nClass {y}: already has {have} >= {target_per_class}, skipping.\")\n",
        "            continue\n",
        "\n",
        "        vids = originals_by_label[y]\n",
        "        if len(vids) == 0:\n",
        "            print(f\"\\nClass {y}: no originals, cannot augment.\")\n",
        "            continue\n",
        "\n",
        "        print(f\"\\nClass {y}: need {need} more to reach {target_per_class}.\")\n",
        "\n",
        "        # round-robin across originals so you don't over-augment just one clip\n",
        "        for i in range(need):\n",
        "            src_vid = vids[i % len(vids)]\n",
        "            stem = src_vid.stem  # e.g. \"4_pushup_abc\"\n",
        "            aug_id = rng.randint(0, 10**9)\n",
        "\n",
        "            out_name = f\"{stem}__balaug_{aug_id}.avi\"\n",
        "            out_path = dst_dir / out_name\n",
        "\n",
        "            # avoid extremely rare collision\n",
        "            while out_path.exists():\n",
        "                aug_id = rng.randint(0, 10**9)\n",
        "                out_name = f\"{stem}__balaug_{aug_id}.avi\"\n",
        "                out_path = dst_dir / out_name\n",
        "\n",
        "            augment_and_save_video(\n",
        "                in_path=str(src_vid),\n",
        "                out_path=str(out_path),\n",
        "                crop_scale_range=crop_scale_range,\n",
        "                flip_prob=flip_prob,\n",
        "                rot_range=rot_range,\n",
        "                scale_range=scale_range,\n",
        "                trans_frac=trans_frac,\n",
        "                seed=rng.randint(0, 10**9),\n",
        "            )\n",
        "\n",
        "        print(f\"  -> class {y} now should be ~{target_per_class} in {dst_dir}\")\n",
        "\n",
        "    print(f\"\\n✅ Balanced augmented dataset written to: {dst_dir}\")\n",
        "    return str(dst_dir)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q5ATmQSPox3W"
      },
      "source": [
        "## Cached Pose Data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "t5glyUbLox3W"
      },
      "outputs": [],
      "source": [
        "from torch.utils.data import WeightedRandomSampler\n",
        "\n",
        "\n",
        "class CachedPoseDataset(Dataset):\n",
        "    \"\"\"\n",
        "    Pose-only dataset.\n",
        "    Expects files like: 4_something__aug1.npy\n",
        "    Label = int(prefix before first underscore)\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, pose_dir):\n",
        "        self.pose_dir = Path(pose_dir)\n",
        "        self.pose_files = sorted(self.pose_dir.glob(\"*.npy\"))\n",
        "\n",
        "        if len(self.pose_files) == 0:\n",
        "            raise RuntimeError(f\"No .npy files found in {pose_dir}\")\n",
        "\n",
        "        self.labels = [int(p.name.split(\"_\")[0]) for p in self.pose_files]\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.pose_files)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        p = self.pose_files[idx]\n",
        "        kp = np.load(p).astype(np.float32)          # (T,33,3)\n",
        "        kp = torch.from_numpy(kp).float()\n",
        "        y = torch.tensor(self.labels[idx], dtype=torch.long)\n",
        "        return kp, y\n",
        "\n",
        "\n",
        "def resample_pose_to_length(pose: np.ndarray, target_len: int = 300) -> np.ndarray:\n",
        "    \"\"\"\n",
        "    pose: (T, 33, 3)\n",
        "    returns: (target_len, 33, 3)\n",
        "    \"\"\"\n",
        "    T, V, D = pose.shape\n",
        "    if T == target_len:\n",
        "        return pose.astype(np.float32)\n",
        "\n",
        "    # Old and new time indices\n",
        "    old_x = np.linspace(0, 1, T)\n",
        "    new_x = np.linspace(0, 1, target_len)\n",
        "\n",
        "    out = np.zeros((target_len, V, D), dtype=np.float32)\n",
        "    for v in range(V):\n",
        "        for d in range(D):\n",
        "            out[:, v, d] = np.interp(new_x, old_x, pose[:, v, d])\n",
        "\n",
        "    return out\n",
        "\n",
        "\n",
        "def pose_collate_fn(batch):\n",
        "    kps, labels = zip(*batch)\n",
        "    return torch.stack(kps, dim=0), torch.stack(labels, dim=0)\n",
        "\n",
        "\n",
        "def get_dataloaders(pose_dir, batch_size=4, val_split=0.01, frame_size=(224, 224)):\n",
        "    \"\"\"Create train and validation dataloaders.\"\"\"\n",
        "\n",
        "    full_dataset = CachedPoseDataset(pose_dir)\n",
        "    val_size = int(len(full_dataset) * val_split)\n",
        "    train_size = len(full_dataset) - val_size\n",
        "\n",
        "    train_dataset, val_dataset = random_split(\n",
        "        full_dataset,\n",
        "        [train_size, val_size],\n",
        "        generator=torch.Generator().manual_seed(42)\n",
        "    )\n",
        "\n",
        "    train_loader = DataLoader(\n",
        "        train_dataset,\n",
        "        batch_size=batch_size,\n",
        "        shuffle=True,\n",
        "        num_workers=0,\n",
        "        collate_fn=pose_collate_fn\n",
        "    )\n",
        "\n",
        "    val_loader = DataLoader(\n",
        "        val_dataset,\n",
        "        batch_size=batch_size,\n",
        "        shuffle=False,\n",
        "        num_workers=0,\n",
        "        collate_fn=pose_collate_fn\n",
        "    )\n",
        "\n",
        "    print(\n",
        "        f\"Train: {len(train_dataset)} videos, Val: {len(val_dataset)} videos\\n\")\n",
        "\n",
        "    return train_loader, val_loader\n",
        "\n",
        "\n",
        "video_dir = './video-data'"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YVPYRadrZdty"
      },
      "source": [
        "# Create a Model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UrpSDGMWaBR3"
      },
      "source": [
        "For this assignment, we request you use PyTorch. Below is an example of how to instantiate a very basic PyTorch model.\n",
        "\n",
        "Note, this model below needs a _lot_ of work.\n",
        "\n",
        "Please include your code for creating your model below.\n",
        "\n",
        "The only constraint here is that you define a Python object which inherits from a PyTorch nn.Module object. Beyond that, please feel free to implement anything you like: Transformer, Vision Transformer, MLP, CNN, etc."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sRolEQeAbxsx"
      },
      "source": [
        "### TODO 4\n",
        "\n",
        "Create your model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "H5FlYz3paNxu"
      },
      "outputs": [],
      "source": [
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from huggingface_hub import HfApi, hf_hub_download\n",
        "\n",
        "\n",
        "####################\n",
        "# Video Classifier\n",
        "####################\n",
        "\n",
        "# This one was initially used during development\n",
        "class PoseTemporalCNN(nn.Module):\n",
        "    def __init__(self, num_classes=10, num_joints=33, dim=3):\n",
        "        super().__init__()\n",
        "        in_feats = num_joints * dim\n",
        "\n",
        "        self.net = nn.Sequential(\n",
        "            nn.Conv1d(in_feats, 256, kernel_size=5, padding=2),\n",
        "            nn.ReLU(),\n",
        "            nn.Conv1d(256, 256, kernel_size=5, padding=2),\n",
        "            nn.ReLU(),\n",
        "            nn.AdaptiveAvgPool1d(1),\n",
        "        )\n",
        "        self.fc = nn.Linear(256, num_classes)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # x: (B, T, 33, 3)\n",
        "        B, T, V, D = x.shape\n",
        "        x = x.view(B, T, V * D).transpose(1, 2)  # (B, V*D, T)\n",
        "        x = self.net(x).squeeze(-1)              # (B, 256)\n",
        "        return self.fc(x)\n",
        "\n",
        "# This is the one we actually use\n",
        "class PoseGRUClassifier(nn.Module):\n",
        "    def __init__(self, num_classes=11, input_dim=99, hidden=256):\n",
        "        super().__init__()\n",
        "        self.gru = nn.GRU(input_size=input_dim, hidden_size=hidden,\n",
        "                          batch_first=True, bidirectional=True)\n",
        "        self.head = nn.Sequential(\n",
        "            nn.Linear(hidden * 2, 256),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(0.4),\n",
        "            nn.Linear(256, num_classes),\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        # x: (B, T, 33, 3)\n",
        "        B, T, V, D = x.shape\n",
        "        x = x.view(B, T, V * D)  # (B, T, 99)\n",
        "\n",
        "        out, _ = self.gru(x)\n",
        "        vid = out.mean(dim=1)\n",
        "        return self.head(vid)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qH-VV9ccox3X"
      },
      "source": [
        "## Cache Pose Data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "17_CUgdOox3X"
      },
      "outputs": [],
      "source": [
        "from pathlib import Path\n",
        "\n",
        "\n",
        "def cache_all_poses(video_dir=\"./video-data\", pose_dir=\"./pose-data\",\n",
        "                    target_frames=300, stride=2, include_visibility=True):\n",
        "\n",
        "    video_dir = Path(video_dir)\n",
        "    pose_dir = Path(pose_dir)\n",
        "    pose_dir.mkdir(exist_ok=True)\n",
        "\n",
        "    extractor = MediaPipePoseExtractor(\n",
        "        model_complexity=1, smooth_landmarks=True)\n",
        "\n",
        "    for vid in video_dir.iterdir():\n",
        "        if vid.suffix.lower() not in [\".mp4\", \".avi\", \".mov\"]:\n",
        "            continue\n",
        "\n",
        "        out = pose_dir / (vid.stem + \".npy\")\n",
        "        if out.exists():\n",
        "            continue\n",
        "\n",
        "        print(\"Caching\", vid.name)\n",
        "\n",
        "        kp = extractor.extract(str(vid), max_frames=target_frames,\n",
        "                               stride=stride, include_visibility=include_visibility)\n",
        "\n",
        "        kp = resample_pose_to_length(kp, target_len=target_frames)\n",
        "\n",
        "        np.save(out, kp.astype(np.float32))\n",
        "\n",
        "\n",
        "def base_id_from_filename(fname: str) -> str:\n",
        "    # \"4_pushup_001__aug0.mp4\" -> \"4_pushup_001\"\n",
        "    stem = os.path.splitext(fname)[0]\n",
        "    stem = re.sub(r\"__aug\\d+$\", \"\", stem)\n",
        "    return stem"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IIOeSsQHox3X"
      },
      "source": [
        "## Pose Stitching for Videos of 8,9 and 10"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "FSbBpgDkox3X"
      },
      "outputs": [],
      "source": [
        "\n",
        "from pathlib import Path\n",
        "import numpy as np\n",
        "import shutil\n",
        "\n",
        "\n",
        "def self_stitch_pose(pose: np.ndarray, times: int, fade: int = 15) -> np.ndarray:\n",
        "    \"\"\"\n",
        "    Repeat the same pose sequence `times` and cross-fade at boundaries.\n",
        "    pose: (T, 33, 3)\n",
        "    \"\"\"\n",
        "    assert times >= 1\n",
        "    pose = pose.astype(np.float32)\n",
        "\n",
        "    out = pose\n",
        "    for _ in range(times - 1):\n",
        "        nxt = pose\n",
        "\n",
        "        f = min(fade, len(out), len(nxt))\n",
        "        if f <= 1:\n",
        "            out = np.concatenate([out, nxt], axis=0)\n",
        "            continue\n",
        "\n",
        "        a = out[:-f]\n",
        "        b1 = out[-f:]\n",
        "        b2 = nxt[:f]\n",
        "        c = nxt[f:]\n",
        "\n",
        "        w = np.linspace(0, 1, f, dtype=np.float32)[:, None, None]\n",
        "        blended = (1 - w) * b1 + w * b2\n",
        "\n",
        "        out = np.concatenate([a, blended, c], axis=0)\n",
        "\n",
        "    return out\n",
        "\n",
        "\n",
        "def build_training_pose_dir_with_selfstitch(\n",
        "    base_pose_dir=\"./pose-data-aug\",\n",
        "    out_pose_dir=\"./pose-data-train\",\n",
        "    fade=15,\n",
        "    target_frames=300,\n",
        "    add_noise_std=0.0015,\n",
        "    seed=42,\n",
        "    overwrite=False,\n",
        "):\n",
        "    \"\"\"\n",
        "    Creates a new pose directory for training:\n",
        "      - copies ALL .npy files from base_pose_dir into out_pose_dir\n",
        "      - generates self-stitched samples (3->9, 4->8, 5->10) from the base poses\n",
        "        and writes them into out_pose_dir as additional .npy files\n",
        "\n",
        "    Result: out_pose_dir becomes the single directory you train on.\n",
        "    \"\"\"\n",
        "    rng = np.random.default_rng(seed)\n",
        "\n",
        "    base_pose_dir = Path(base_pose_dir)\n",
        "    out_pose_dir = Path(out_pose_dir)\n",
        "    out_pose_dir.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "    # Optional: clear output dir first\n",
        "    if overwrite:\n",
        "        for p in out_pose_dir.glob(\"*.npy\"):\n",
        "            p.unlink()\n",
        "\n",
        "    # 1) Copy all base poses into out_pose_dir\n",
        "    base_files = sorted(base_pose_dir.glob(\"*.npy\"))\n",
        "    if not base_files:\n",
        "        raise RuntimeError(f\"No .npy files found in {base_pose_dir}\")\n",
        "\n",
        "    copied = 0\n",
        "    for p in base_files:\n",
        "        dst = out_pose_dir / p.name\n",
        "        if not dst.exists():\n",
        "            shutil.copy2(p, dst)\n",
        "            copied += 1\n",
        "\n",
        "    print(f\"✅ Copied {copied} base pose files into {out_pose_dir}\")\n",
        "\n",
        "    # 2) Generate self-stitched poses into the SAME out_pose_dir\n",
        "    # base reps -> (repeat times, target reps)\n",
        "    plan = {3: (3, 9), 4: (2, 8), 5: (2, 10)}\n",
        "    written = {8: 0, 9: 0, 10: 0}\n",
        "\n",
        "    for p in base_files:\n",
        "        base_reps = int(p.name.split(\"_\")[0])\n",
        "        if base_reps not in plan:\n",
        "            continue\n",
        "\n",
        "        times, target_reps = plan[base_reps]\n",
        "\n",
        "        kp = np.load(p).astype(np.float32)  # (T,33,3)\n",
        "\n",
        "        stitched = self_stitch_pose(kp, times=times, fade=fade)\n",
        "\n",
        "        if add_noise_std and add_noise_std > 0:\n",
        "            stitched += rng.normal(0, add_noise_std,\n",
        "                                   size=stitched.shape).astype(np.float32)\n",
        "\n",
        "        stitched = resample_pose_to_length(stitched, target_len=target_frames)\n",
        "\n",
        "        # Name it with new target reps prefix so your dataset label becomes 8/9/10\n",
        "        out_name = f\"{target_reps}_\" + p.stem + f\"__selfx{times}.npy\"\n",
        "        dst = out_pose_dir / out_name\n",
        "\n",
        "        # avoid overwriting if rerun\n",
        "        if dst.exists():\n",
        "            continue\n",
        "\n",
        "        np.save(dst, stitched.astype(np.float32))\n",
        "        written[target_reps] += 1\n",
        "\n",
        "    print(f\"✅ Added self-stitched poses into {out_pose_dir}\")\n",
        "    print(f\"   +8 reps:  {written[8]}\")\n",
        "    print(f\"   +9 reps:  {written[9]}\")\n",
        "    print(f\"   +10 reps: {written[10]}\")\n",
        "\n",
        "    return str(out_pose_dir)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6FWIY53zox3X"
      },
      "source": [
        "## Balancing through Copying"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "5kfO787lox3X"
      },
      "outputs": [],
      "source": [
        "from pathlib import Path\n",
        "import shutil\n",
        "import random\n",
        "from collections import defaultdict\n",
        "\n",
        "\n",
        "def build_pose_dataset_balanced_by_copy(\n",
        "    src_pose_dir=\"./pose-data-aug\",\n",
        "    dst_pose_dir=\"./pose-data-balanced\",\n",
        "    target_per_class=10,\n",
        "    seed=42,\n",
        "    copy_originals=True,\n",
        "):\n",
        "    \"\"\"\n",
        "    Balance pose dataset by COPYING existing pose files (no augmentation).\n",
        "\n",
        "    - Groups .npy files by label (prefix number)\n",
        "    - Copies originals into dst_pose_dir (optional)\n",
        "    - If a class has < target_per_class files, duplicates them round-robin\n",
        "    - If a class has >= target_per_class files, does nothing\n",
        "    \"\"\"\n",
        "    rng = random.Random(seed)\n",
        "\n",
        "    src_pose_dir = Path(src_pose_dir)\n",
        "    dst_pose_dir = Path(dst_pose_dir)\n",
        "    dst_pose_dir.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "    # --- gather source poses by class ---\n",
        "    poses_by_label = defaultdict(list)\n",
        "    for p in sorted(src_pose_dir.glob(\"*.npy\")):\n",
        "        try:\n",
        "            y = int(p.name.split(\"_\")[0])\n",
        "        except Exception:\n",
        "            continue\n",
        "        poses_by_label[y].append(p)\n",
        "\n",
        "    if not poses_by_label:\n",
        "        raise RuntimeError(f\"No .npy files found in {src_pose_dir}\")\n",
        "\n",
        "    print(\"Source pose counts per class:\")\n",
        "    for y in sorted(poses_by_label):\n",
        "        print(f\"  {y}: {len(poses_by_label[y])}\")\n",
        "\n",
        "    # --- copy originals first ---\n",
        "    if copy_originals:\n",
        "        copied = 0\n",
        "        for y, poses in poses_by_label.items():\n",
        "            for p in poses:\n",
        "                dst = dst_pose_dir / p.name\n",
        "                if not dst.exists():\n",
        "                    shutil.copy2(p, dst)\n",
        "                    copied += 1\n",
        "        print(f\"\\n✅ Copied {copied} original pose files into {dst_pose_dir}\")\n",
        "\n",
        "    # --- count what we have now in dst ---\n",
        "    existing_by_label = defaultdict(list)\n",
        "    for p in dst_pose_dir.glob(\"*.npy\"):\n",
        "        try:\n",
        "            y = int(p.name.split(\"_\")[0])\n",
        "        except Exception:\n",
        "            continue\n",
        "        existing_by_label[y].append(p)\n",
        "\n",
        "    print(\"\\nCounts after copying originals:\")\n",
        "    for y in sorted(poses_by_label):\n",
        "        print(f\"  {y}: {len(existing_by_label[y])}\")\n",
        "\n",
        "    # --- duplicate until target_per_class ---\n",
        "    for y in sorted(poses_by_label):\n",
        "        have = len(existing_by_label[y])\n",
        "        if have >= target_per_class:\n",
        "            print(\n",
        "                f\"\\nClass {y}: already has {have} ≥ {target_per_class}, skipping.\")\n",
        "            continue\n",
        "\n",
        "        need = target_per_class - have\n",
        "        src_poses = poses_by_label[y]\n",
        "\n",
        "        print(f\"\\nClass {y}: need {need} more copies.\")\n",
        "\n",
        "        for i in range(need):\n",
        "            src_p = src_poses[i % len(src_poses)]\n",
        "            dup_id = rng.randint(0, 10**9)\n",
        "\n",
        "            out_name = f\"{src_p.stem}__copy{dup_id}.npy\"\n",
        "            out_path = dst_pose_dir / out_name\n",
        "\n",
        "            while out_path.exists():\n",
        "                dup_id = rng.randint(0, 10**9)\n",
        "                out_name = f\"{src_p.stem}__copy{dup_id}.npy\"\n",
        "                out_path = dst_pose_dir / out_name\n",
        "\n",
        "            shutil.copy2(src_p, out_path)\n",
        "            existing_by_label[y].append(out_path)\n",
        "\n",
        "        print(f\"  -> class {y} now has {len(existing_by_label[y])}\")\n",
        "\n",
        "    print(f\"\\n✅ Balanced pose dataset written to: {dst_pose_dir}\")\n",
        "    return str(dst_pose_dir)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3Bou97f8czAu"
      },
      "source": [
        "# Train your Model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VtSqu_ZkcFxs"
      },
      "source": [
        "### TODO 5\n",
        "\n",
        "Training time! Please include your training code below.\n",
        "\n",
        "As per above, please feel free (and encouraged) to rip out all of the below code and replace with your (much better) code.\n",
        "\n",
        "The below should just be used as an example to get you started."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "EueH4HSdcLlE"
      },
      "outputs": [],
      "source": [
        "import torch.optim as optim\n",
        "\n",
        "\n",
        "def train_epoch(model, train_loader, optimizer, criterion, device):\n",
        "    model.train()\n",
        "    total_loss = 0\n",
        "    correct = 0\n",
        "    total = 0\n",
        "\n",
        "    for batch_idx, (data, target) in enumerate(train_loader):\n",
        "        data, target = data.to(device), target.to(device)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        output = model(data)\n",
        "        loss = criterion(output, target)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        total_loss += loss.item()\n",
        "        pred = output.argmax(dim=1)\n",
        "        correct += pred.eq(target).sum().item()\n",
        "        total += target.size(0)\n",
        "\n",
        "    return total_loss / len(train_loader), correct / total\n",
        "\n",
        "\n",
        "def evaluate(model, test_loader, criterion, device):\n",
        "    model.eval()\n",
        "    total_loss = 0\n",
        "    correct = 0\n",
        "    total = 0\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for data, target in test_loader:\n",
        "            data, target = data.to(device), target.to(device)\n",
        "            output = model(data)\n",
        "            total_loss += criterion(output, target).item()\n",
        "            pred = output.argmax(dim=1)\n",
        "            correct += pred.eq(target).sum().item()\n",
        "            total += target.size(0)\n",
        "\n",
        "    return total_loss / len(test_loader), correct / total\n",
        "\n",
        "\n",
        "def train_model(epochs=5, lr=1e-3, pose_dir=\"./pose-data\"):\n",
        "    \"\"\"Train and return your model.\"\"\"\n",
        "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "    print(f\"Using device: {device}\\n\")\n",
        "\n",
        "    # model = PoseTemporalCNN(num_classes=10, num_joints=33, dim=3).to(device)\n",
        "    model = PoseGRUClassifier(\n",
        "        num_classes=11, input_dim=99, hidden=256).to(device)\n",
        "\n",
        "    print(\"Instantiated model.\\n\")\n",
        "    optimizer = optim.Adam(model.parameters(), lr=lr)\n",
        "    scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(\n",
        "        optimizer, mode=\"min\", factor=0.9, patience=10\n",
        "    )\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "    train_loader, val_loader = get_dataloaders(\n",
        "        pose_dir=pose_dir, batch_size=32, val_split=0.01)\n",
        "    print(\"Got dataloaders.\\n\")\n",
        "\n",
        "    print(\"Go time. Let the training commence.\\n\")\n",
        "\n",
        "    print(\"Validation Accuracy is only a dummy value, since val_split=0.01 and model is trained on entire dataset for maximum performance\")\n",
        "\n",
        "    for epoch in range(1, epochs + 1):\n",
        "\n",
        "        train_loss, train_acc = train_epoch(\n",
        "            model, train_loader, optimizer, criterion, device)\n",
        "        val_loss, val_acc = evaluate(model, val_loader, criterion, device)\n",
        "\n",
        "        scheduler.step(val_loss)\n",
        "\n",
        "        print(f\"Epoch {epoch}/{epochs} | \"\n",
        "              f\"Train Loss: {train_loss:.4f}, Train Acc: {train_acc:.4f} | \"\n",
        "              f\"Val Loss: {val_loss:.4f}, Val Acc: {val_acc:.4f}, LR: {optimizer.param_groups[0]['lr']:.6f}\")\n",
        "\n",
        "    return model\n",
        "\n",
        "\n",
        "\n",
        "def save_model(model, path=\"model.pt\"):\n",
        "    \"\"\"Save the model weights to a file.\"\"\"\n",
        "    torch.save(model.state_dict(), path)\n",
        "    print(f\"Model saved to {path}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JiCpqcEtox3Y"
      },
      "source": [
        "## Build Augmented Dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "G789GwnOox3Y",
        "outputId": "3e076eeb-b9a5-4a34-9478-a459cd21209e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found 77 original videos\n",
            "Augmented: 1_dksksjfwijf.mp4\n",
            "Augmented: 2_dfsaeklnvvalkej.mp4\n",
            "Augmented: 2_difficult_2.mp4\n",
            "Augmented: 2_difficult_sdafkljsalkfj.mp4\n",
            "Augmented: 2_dkdjwkndkfw.mp4\n",
            "Augmented: 2_dkdmkejkeimdh.mp4\n",
            "Augmented: 2_dkjd823kjf.mp4\n",
            "Augmented: 2_dsalkfjalwkenlke.mp4\n",
            "Augmented: 2_kling_20251205_Text_to_Video_On_a_sandy_4976_0.mp4\n",
            "Augmented: 2_kling_20251206_Text_to_Video_Generate_a_71_1.mp4\n",
            "Augmented: 2_sadfasjldkfjaseifj.mp4\n",
            "Augmented: 2_sdafkjaslkclaksdjkas.mp4\n",
            "Augmented: 2_sdfkjsaleijflaskdjf.mp4\n",
            "Augmented: 2_sdjfhafsldkjhjk.mp4\n",
            "Augmented: 2_sdkjdsflkjfwa.mp4\n",
            "Augmented: 2_sdlfjlewlkjkj.mp4\n",
            "Augmented: 2_sdlkjsaelijfksdjf.mp4\n",
            "Augmented: 3_asldkfjalwieaskdfaskdf.mp4\n",
            "Augmented: 3_dkk873lkjlksajdf.mp4\n",
            "Augmented: 3_dsjlaeijlksjdfie.mp4\n",
            "Augmented: 3_dsksdfjbvsdkj.mp4\n",
            "Augmented: 3_dslkaldskjflakjs.mp4\n",
            "Augmented: 3_ewdfkjwaeoihjlkasdjf.mp4\n",
            "Augmented: 3_kling_20251205_Text_to_Video_In_a_grass_4697_0.mp4\n",
            "Augmented: 3_kling_20251205_Text_to_Video_On_a_playg_5028_0.mp4\n",
            "Augmented: 3_kling_20251205_Text_to_Video_On_a_playg_5064_0.mp4\n",
            "Augmented: 3_kling_20251206_Text_to_Video_Generate_a_17_0.mp4\n",
            "Augmented: 3_kling_20251206_Text_to_Video_Generate_a_315_0.mp4\n",
            "Augmented: 3_kling_20251206_Text_to_Video_Generate_a_315_2.mp4\n",
            "Augmented: 3_kling_20251206_Text_to_Video_Generate_a_712_3.mp4\n",
            "Augmented: 3_kling_20251206_Text_to_Video_Generate_a_71_0.mp4\n",
            "Augmented: 3_kling_20251206_Text_to_Video_Generate_a_71_2.mp4\n",
            "Augmented: 3_kling_20251206_Text_to_Video_Generate_a_71_3.mp4\n",
            "Augmented: 3_kling_20251209_Image_to_Video_Generate_a_613_1.mp4\n",
            "Augmented: 3_kling_20251209_Image_to_Video_Generate_a_635_0.mp4\n",
            "Augmented: 3_kling_20251209_Text_to_Video_Generate_a_190_1.mp4\n",
            "Augmented: 3_kling_20251209_Text_to_Video_Generate_a_403_1.mp4\n",
            "Augmented: 3_kling_20251209_Text_to_Video_Generate_a_491_0.mp4\n",
            "Augmented: 3_kling_20251209_Text_to_Video_Generate_a_491_1.mp4\n",
            "Augmented: 3_kling_20251209_Text_to_Video_Generate_a_491_2.mp4\n",
            "Augmented: 3_kling_dskfseu.mp4\n",
            "Augmented: 3_kling_kdjflaskdjf.mp4\n",
            "Augmented: 3_sadklfjasbnlkjlfkj.mp4\n",
            "Augmented: 3_sadlfkjasldkfjasleijlkjfd.mp4\n",
            "Augmented: 3_sadlfkjawelnflksdjf.mp4\n",
            "Augmented: 3_sdfjwaiejflkasjdf.mp4\n",
            "Augmented: 3_sdflkjliejkjdf.mp4\n",
            "Augmented: 3_sdlkfjaleknaksej.mp4\n",
            "Augmented: 3_sdlkfjalkjejafe.mp4\n",
            "Augmented: 3_sdlkjfaslkjfalskjdf.mp4\n",
            "Augmented: 3_sdlkjslndflkseijlkjef.mp4\n",
            "Augmented: 4_20251209_Text_to_Video_Generate_a_561_0.mp4\n",
            "Augmented: 4_asdlkfjalsflnekj.mp4\n",
            "Augmented: 4_aslkcasckmwlejk.mp4\n",
            "Augmented: 4_aslkjasmcalkewjlkje.mp4\n",
            "Augmented: 4_dssalsdkfjweijf.mp4\n",
            "Augmented: 4_kling_20251206_Text_to_Video_Generate_a_28_0.mp4\n",
            "Augmented: 4_kling_20251206_Text_to_Video_Generate_a_315_3.mp4\n",
            "Augmented: 4_kling_20251206_Text_to_Video_Generate_a_58_0.mp4\n",
            "Augmented: 4_kling_20251207_Text_to_Video_Generate_a_521_1.mp4\n",
            "Augmented: 4_kling_20251209_Image_to_Video_Generate_a_635_1.mp4\n",
            "Augmented: 4_kling_20251209_Text_to_Video_Generate_a_190_0.mp4\n",
            "Augmented: 4_kling_20251209_Text_to_Video_Generate_a_218_0.mp4\n",
            "Augmented: 4_kling_20251209_Text_to_Video_Generate_a_263_1.mp4\n",
            "Augmented: 4_kling_20251209_Text_to_Video_Generate_a_377_1.mp4\n",
            "Augmented: 4_kling_20251209_Text_to_Video_Generate_a_452_0.mp4\n",
            "Augmented: 4_kling_20251209_Text_to_Video_Generate_a_452_1.mp4\n",
            "Augmented: 4_kling_20251209_Text_to_Video_Generate_a_561_1.mp4\n",
            "Augmented: 4_kling_20251209_Text_to_Video_Generate_a_588_2.mp4\n",
            "Augmented: 4_pushup_1f2da596-7619-4d55-9376-069e15a42a1a_h264.mp4\n",
            "Augmented: 4_sadflkjasldkjfalseij.mp4\n",
            "Augmented: 4_sadlfkjlknewkjejk.mp4\n",
            "Augmented: 5_sadfjhaslfkjasdlkfjsa.mp4\n",
            "Augmented: 5_sdfkljweoijlkjdsflkjweaij.mp4\n",
            "Augmented: 6_dfjewaijsldkjfsaef.mp4\n",
            "Augmented: 6_kling_20251209_Text_to_Video_Generate_a_218_1.mp4\n",
            "Augmented: 7_sadkjfkljekj.mp4\n",
            "Expanded dataset written to: video-data-aug (originals + aug copies)\n"
          ]
        }
      ],
      "source": [
        "\n",
        "\n",
        "expanded_dir = build_augmented_dataset(\n",
        "    src_dir=\"./video-data\",\n",
        "    dst_dir=\"./video-data-aug\",\n",
        "    copies_per_video=2,\n",
        "    crop_scale_range=(0.80, 1.00),\n",
        "    flip_prob=0.5,\n",
        ")\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BmSqbMeWox3Y"
      },
      "source": [
        "## Cache Pose Data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "WIEPTeoiox3Y",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3efb2d08-26cb-4ad9-f9c9-ce05541bd33e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Caching 2_dkdjwkndkfw__aug1.avi\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/google/protobuf/symbol_database.py:55: UserWarning: SymbolDatabase.GetPrototype() is deprecated. Please use message_factory.GetMessageClass() instead. SymbolDatabase.GetPrototype() will be removed soon.\n",
            "  warnings.warn('SymbolDatabase.GetPrototype() is deprecated. Please '\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Caching 1_dksksjfwijf.mp4\n",
            "Caching 3_asldkfjalwieaskdfaskdf.mp4\n",
            "Caching 7_sadkjfkljekj__aug1.avi\n",
            "Caching 3_dkk873lkjlksajdf__aug0.avi\n",
            "Caching 3_asldkfjalwieaskdfaskdf__aug0.avi\n",
            "Caching 3_kling_20251209_Text_to_Video_Generate_a_491_1.mp4\n",
            "Caching 3_dsjlaeijlksjdfie__aug0.avi\n",
            "Caching 3_kling_20251205_Text_to_Video_On_a_playg_5064_0__aug1.avi\n",
            "Caching 3_ewdfkjwaeoihjlkasdjf.mp4\n",
            "Caching 3_sdlkfjaleknaksej.mp4\n",
            "Caching 4_kling_20251207_Text_to_Video_Generate_a_521_1__aug0.avi\n",
            "Caching 5_sdfkljweoijlkjdsflkjweaij.mp4\n",
            "Caching 3_kling_20251209_Text_to_Video_Generate_a_403_1.mp4\n",
            "Caching 3_sadlfkjawelnflksdjf.mp4\n",
            "Caching 2_kling_20251206_Text_to_Video_Generate_a_71_1.mp4\n",
            "Caching 2_dsalkfjalwkenlke.mp4\n",
            "Caching 4_kling_20251209_Text_to_Video_Generate_a_263_1__aug0.avi\n",
            "Caching 4_kling_20251209_Text_to_Video_Generate_a_561_1__aug0.avi\n",
            "Caching 2_sadfasjldkfjaseifj.mp4\n",
            "Caching 4_sadflkjasldkjfalseij__aug0.avi\n",
            "Caching 2_difficult_2.mp4\n",
            "Caching 3_kling_20251205_Text_to_Video_On_a_playg_5028_0__aug1.avi\n",
            "Caching 3_sdlkjfaslkjfalskjdf__aug1.avi\n",
            "Caching 3_kling_20251206_Text_to_Video_Generate_a_315_0__aug1.avi\n",
            "Caching 2_sdlfjlewlkjkj__aug1.avi\n",
            "Caching 3_kling_20251209_Text_to_Video_Generate_a_491_1__aug0.avi\n",
            "Caching 4_20251209_Text_to_Video_Generate_a_561_0.mp4\n",
            "Caching 1_dksksjfwijf__aug1.avi\n",
            "Caching 3_kling_20251209_Text_to_Video_Generate_a_491_2__aug1.avi\n",
            "Caching 2_dkdjwkndkfw.mp4\n",
            "Caching 4_kling_20251209_Image_to_Video_Generate_a_635_1__aug1.avi\n",
            "Caching 2_sdkjdsflkjfwa__aug0.avi\n",
            "Caching 2_dsalkfjalwkenlke__aug1.avi\n",
            "Caching 3_kling_20251206_Text_to_Video_Generate_a_315_0__aug0.avi\n",
            "Caching 2_sdafkjaslkclaksdjkas__aug0.avi\n",
            "Caching 3_sadlfkjasldkfjasleijlkjfd__aug1.avi\n",
            "Caching 4_kling_20251209_Image_to_Video_Generate_a_635_1__aug0.avi\n",
            "Caching 3_kling_20251209_Text_to_Video_Generate_a_491_0.mp4\n",
            "Caching 3_sdfjwaiejflkasjdf.mp4\n",
            "Caching 3_kling_20251209_Text_to_Video_Generate_a_190_1__aug0.avi\n",
            "Caching 2_dsalkfjalwkenlke__aug0.avi\n",
            "Caching 3_ewdfkjwaeoihjlkasdjf__aug1.avi\n",
            "Caching 4_kling_20251206_Text_to_Video_Generate_a_28_0__aug1.avi\n",
            "Caching 3_dkk873lkjlksajdf.mp4\n",
            "Caching 4_asdlkfjalsflnekj__aug0.avi\n",
            "Caching 6_dfjewaijsldkjfsaef__aug0.avi\n",
            "Caching 3_kling_20251209_Image_to_Video_Generate_a_635_0.mp4\n",
            "Caching 3_kling_20251205_Text_to_Video_On_a_playg_5028_0.mp4\n",
            "Caching 3_sadlfkjasldkfjasleijlkjfd__aug0.avi\n",
            "Caching 5_sadfjhaslfkjasdlkfjsa__aug1.avi\n",
            "Caching 4_kling_20251209_Text_to_Video_Generate_a_263_1.mp4\n",
            "Caching 4_aslkjasmcalkewjlkje__aug1.avi\n",
            "Caching 4_aslkcasckmwlejk__aug1.avi\n",
            "Caching 5_sdfkljweoijlkjdsflkjweaij__aug0.avi\n",
            "Caching 3_sdlkjslndflkseijlkjef__aug1.avi\n",
            "Caching 4_aslkcasckmwlejk__aug0.avi\n",
            "Caching 4_kling_20251209_Text_to_Video_Generate_a_588_2__aug0.avi\n",
            "Caching 3_kling_20251206_Text_to_Video_Generate_a_71_3.mp4\n",
            "Caching 2_sdkjdsflkjfwa__aug1.avi\n",
            "Caching 4_kling_20251206_Text_to_Video_Generate_a_28_0__aug0.avi\n",
            "Caching 2_dkjd823kjf.mp4\n",
            "Caching 3_sdlkfjaleknaksej__aug1.avi\n",
            "Caching 2_difficult_2__aug1.avi\n",
            "Caching 3_kling_20251209_Text_to_Video_Generate_a_403_1__aug1.avi\n",
            "Caching 2_sdjfhafsldkjhjk__aug0.avi\n",
            "Caching 3_sadlfkjawelnflksdjf__aug0.avi\n",
            "Caching 3_kling_20251209_Image_to_Video_Generate_a_613_1__aug1.avi\n",
            "Caching 3_kling_20251209_Text_to_Video_Generate_a_491_2__aug0.avi\n",
            "Caching 3_kling_20251206_Text_to_Video_Generate_a_71_0.mp4\n",
            "Caching 3_sdflkjliejkjdf__aug1.avi\n",
            "Caching 3_kling_20251206_Text_to_Video_Generate_a_17_0__aug0.avi\n",
            "Caching 3_kling_20251206_Text_to_Video_Generate_a_315_2__aug0.avi\n",
            "Caching 3_kling_20251206_Text_to_Video_Generate_a_71_3__aug0.avi\n",
            "Caching 3_kling_kdjflaskdjf.mp4\n",
            "Caching 4_sadlfkjlknewkjejk.mp4\n",
            "Caching 4_kling_20251209_Text_to_Video_Generate_a_377_1__aug1.avi\n",
            "Caching 4_pushup_1f2da596-7619-4d55-9376-069e15a42a1a_h264.mp4\n",
            "Caching 3_sadlfkjasldkfjasleijlkjfd.mp4\n",
            "Caching 4_kling_20251209_Text_to_Video_Generate_a_452_0__aug1.avi\n",
            "Caching 3_dsksdfjbvsdkj__aug0.avi\n",
            "Caching 4_kling_20251206_Text_to_Video_Generate_a_58_0__aug0.avi\n",
            "Caching 2_dfsaeklnvvalkej__aug0.avi\n",
            "Caching 2_dfsaeklnvvalkej.mp4\n",
            "Caching 2_kling_20251205_Text_to_Video_On_a_sandy_4976_0__aug0.avi\n",
            "Caching 4_asdlkfjalsflnekj.mp4\n",
            "Caching 3_kling_20251206_Text_to_Video_Generate_a_315_2.mp4\n",
            "Caching 3_kling_20251206_Text_to_Video_Generate_a_71_0__aug1.avi\n",
            "Caching 2_sdafkjaslkclaksdjkas__aug1.avi\n",
            "Caching 3_kling_20251206_Text_to_Video_Generate_a_71_0__aug0.avi\n",
            "Caching 4_sadlfkjlknewkjejk__aug0.avi\n",
            "Caching 3_kling_20251209_Text_to_Video_Generate_a_403_1__aug0.avi\n",
            "Caching 4_kling_20251206_Text_to_Video_Generate_a_58_0.mp4\n",
            "Caching 4_kling_20251209_Text_to_Video_Generate_a_377_1__aug0.avi\n",
            "Caching 6_dfjewaijsldkjfsaef.mp4\n",
            "Caching 2_dkdjwkndkfw__aug0.avi\n",
            "Caching 2_sadfasjldkfjaseifj__aug1.avi\n",
            "Caching 3_kling_dskfseu.mp4\n",
            "Caching 4_dssalsdkfjweijf.mp4\n",
            "Caching 3_kling_20251209_Text_to_Video_Generate_a_491_0__aug0.avi\n",
            "Caching 4_kling_20251209_Text_to_Video_Generate_a_452_0.mp4\n",
            "Caching 4_kling_20251209_Text_to_Video_Generate_a_561_1.mp4\n",
            "Caching 2_difficult_sdafkljsalkfj__aug1.avi\n",
            "Caching 6_kling_20251209_Text_to_Video_Generate_a_218_1__aug0.avi\n",
            "Caching 4_dssalsdkfjweijf__aug0.avi\n",
            "Caching 3_kling_20251209_Text_to_Video_Generate_a_491_2.mp4\n",
            "Caching 4_20251209_Text_to_Video_Generate_a_561_0__aug0.avi\n",
            "Caching 4_kling_20251209_Text_to_Video_Generate_a_190_0.mp4\n",
            "Caching 3_kling_20251209_Text_to_Video_Generate_a_491_0__aug1.avi\n",
            "Caching 5_sadfjhaslfkjasdlkfjsa__aug0.avi\n",
            "Caching 4_kling_20251209_Text_to_Video_Generate_a_588_2.mp4\n",
            "Caching 4_kling_20251209_Text_to_Video_Generate_a_190_0__aug0.avi\n",
            "Caching 2_difficult_sdafkljsalkfj.mp4\n",
            "Caching 4_aslkjasmcalkewjlkje.mp4\n",
            "Caching 2_sdkjdsflkjfwa.mp4\n",
            "Caching 3_kling_20251209_Text_to_Video_Generate_a_491_1__aug1.avi\n",
            "Caching 3_kling_20251209_Image_to_Video_Generate_a_635_0__aug0.avi\n",
            "Caching 6_dfjewaijsldkjfsaef__aug1.avi\n",
            "Caching 4_kling_20251206_Text_to_Video_Generate_a_28_0.mp4\n",
            "Caching 4_kling_20251209_Text_to_Video_Generate_a_218_0__aug1.avi\n",
            "Caching 3_kling_20251205_Text_to_Video_In_a_grass_4697_0.mp4\n",
            "Caching 2_kling_20251205_Text_to_Video_On_a_sandy_4976_0__aug1.avi\n",
            "Caching 3_kling_20251205_Text_to_Video_In_a_grass_4697_0__aug0.avi\n",
            "Caching 3_dsjlaeijlksjdfie__aug1.avi\n",
            "Caching 3_kling_dskfseu__aug1.avi\n",
            "Caching 4_20251209_Text_to_Video_Generate_a_561_0__aug1.avi\n",
            "Caching 2_dkdmkejkeimdh__aug0.avi\n",
            "Caching 2_sdfkjsaleijflaskdjf__aug0.avi\n",
            "Caching 3_sdlkjslndflkseijlkjef__aug0.avi\n",
            "Caching 4_kling_20251209_Text_to_Video_Generate_a_263_1__aug1.avi\n",
            "Caching 3_sdflkjliejkjdf__aug0.avi\n",
            "Caching 3_kling_20251205_Text_to_Video_On_a_playg_5064_0.mp4\n",
            "Caching 2_sdafkjaslkclaksdjkas.mp4\n",
            "Caching 7_sadkjfkljekj__aug0.avi\n",
            "Caching 6_kling_20251209_Text_to_Video_Generate_a_218_1__aug1.avi\n",
            "Caching 4_kling_20251206_Text_to_Video_Generate_a_315_3.mp4\n",
            "Caching 2_sdlfjlewlkjkj.mp4\n",
            "Caching 4_sadflkjasldkjfalseij__aug1.avi\n",
            "Caching 3_sdlkjfaslkjfalskjdf__aug0.avi\n",
            "Caching 4_kling_20251206_Text_to_Video_Generate_a_315_3__aug0.avi\n",
            "Caching 4_aslkjasmcalkewjlkje__aug0.avi\n",
            "Caching 3_kling_20251205_Text_to_Video_On_a_playg_5028_0__aug0.avi\n",
            "Caching 2_sadfasjldkfjaseifj__aug0.avi\n",
            "Caching 3_sdfjwaiejflkasjdf__aug0.avi\n",
            "Caching 3_kling_dskfseu__aug0.avi\n",
            "Caching 2_difficult_sdafkljsalkfj__aug0.avi\n",
            "Caching 4_kling_20251209_Text_to_Video_Generate_a_452_1__aug1.avi\n",
            "Caching 2_dkjd823kjf__aug0.avi\n",
            "Caching 4_kling_20251209_Text_to_Video_Generate_a_218_0.mp4\n",
            "Caching 1_dksksjfwijf__aug0.avi\n",
            "Caching 4_asdlkfjalsflnekj__aug1.avi\n",
            "Caching 3_kling_20251206_Text_to_Video_Generate_a_315_0.mp4\n",
            "Caching 4_kling_20251209_Text_to_Video_Generate_a_452_0__aug0.avi\n",
            "Caching 3_dkk873lkjlksajdf__aug1.avi\n",
            "Caching 3_sdlkjfaslkjfalskjdf.mp4\n",
            "Caching 3_sdlkfjaleknaksej__aug0.avi\n",
            "Caching 3_kling_20251205_Text_to_Video_On_a_playg_5064_0__aug0.avi\n",
            "Caching 2_sdlkjsaelijfksdjf__aug0.avi\n",
            "Caching 4_kling_20251209_Text_to_Video_Generate_a_218_0__aug0.avi\n",
            "Caching 3_kling_20251205_Text_to_Video_In_a_grass_4697_0__aug1.avi\n",
            "Caching 2_sdlkjsaelijfksdjf.mp4\n",
            "Caching 4_kling_20251209_Text_to_Video_Generate_a_452_1.mp4\n",
            "Caching 2_dkdmkejkeimdh.mp4\n",
            "Caching 3_kling_20251206_Text_to_Video_Generate_a_712_3__aug0.avi\n",
            "Caching 4_dssalsdkfjweijf__aug1.avi\n",
            "Caching 3_kling_20251209_Text_to_Video_Generate_a_190_1.mp4\n",
            "Caching 3_kling_20251206_Text_to_Video_Generate_a_315_2__aug1.avi\n",
            "Caching 6_kling_20251209_Text_to_Video_Generate_a_218_1.mp4\n",
            "Caching 3_dslkaldskjflakjs__aug1.avi\n",
            "Caching 3_kling_20251209_Image_to_Video_Generate_a_613_1__aug0.avi\n",
            "Caching 3_sadklfjasbnlkjlfkj__aug0.avi\n",
            "Caching 3_sdflkjliejkjdf.mp4\n",
            "Caching 4_kling_20251207_Text_to_Video_Generate_a_521_1.mp4\n",
            "Caching 3_kling_20251206_Text_to_Video_Generate_a_71_2__aug1.avi\n",
            "Caching 3_sadlfkjawelnflksdjf__aug1.avi\n",
            "Caching 3_kling_20251209_Image_to_Video_Generate_a_613_1.mp4\n",
            "Caching 2_sdlkjsaelijfksdjf__aug1.avi\n",
            "Caching 3_kling_20251209_Image_to_Video_Generate_a_635_0__aug1.avi\n",
            "Caching 4_kling_20251207_Text_to_Video_Generate_a_521_1__aug1.avi\n",
            "Caching 2_dfsaeklnvvalkej__aug1.avi\n",
            "Caching 2_sdfkjsaleijflaskdjf.mp4\n",
            "Caching 4_kling_20251209_Text_to_Video_Generate_a_377_1.mp4\n",
            "Caching 4_kling_20251206_Text_to_Video_Generate_a_58_0__aug1.avi\n",
            "Caching 3_sdlkfjalkjejafe.mp4\n",
            "Caching 2_dkdmkejkeimdh__aug1.avi\n",
            "Caching 2_sdlfjlewlkjkj__aug0.avi\n",
            "Caching 3_asldkfjalwieaskdfaskdf__aug1.avi\n",
            "Caching 2_sdjfhafsldkjhjk__aug1.avi\n",
            "Caching 7_sadkjfkljekj.mp4\n",
            "Caching 4_kling_20251209_Text_to_Video_Generate_a_588_2__aug1.avi\n",
            "Caching 3_kling_20251206_Text_to_Video_Generate_a_71_2__aug0.avi\n",
            "Caching 2_dkjd823kjf__aug1.avi\n",
            "Caching 4_sadlfkjlknewkjejk__aug1.avi\n",
            "Caching 4_aslkcasckmwlejk.mp4\n",
            "Caching 3_sdlkjslndflkseijlkjef.mp4\n",
            "Caching 5_sdfkljweoijlkjdsflkjweaij__aug1.avi\n",
            "Caching 3_kling_20251206_Text_to_Video_Generate_a_71_3__aug1.avi\n",
            "Caching 4_kling_20251209_Text_to_Video_Generate_a_561_1__aug1.avi\n",
            "Caching 2_kling_20251206_Text_to_Video_Generate_a_71_1__aug1.avi\n",
            "Caching 3_kling_20251206_Text_to_Video_Generate_a_71_2.mp4\n",
            "Caching 4_kling_20251209_Text_to_Video_Generate_a_452_1__aug0.avi\n",
            "Caching 3_dsjlaeijlksjdfie.mp4\n",
            "Caching 3_kling_20251209_Text_to_Video_Generate_a_190_1__aug1.avi\n",
            "Caching 3_sadklfjasbnlkjlfkj__aug1.avi\n",
            "Caching 2_difficult_2__aug0.avi\n",
            "Caching 3_dslkaldskjflakjs__aug0.avi\n",
            "Caching 3_dsksdfjbvsdkj.mp4\n",
            "Caching 4_sadflkjasldkjfalseij.mp4\n",
            "Caching 3_kling_20251206_Text_to_Video_Generate_a_712_3__aug1.avi\n",
            "Caching 3_kling_20251206_Text_to_Video_Generate_a_712_3.mp4\n",
            "Caching 4_pushup_1f2da596-7619-4d55-9376-069e15a42a1a_h264__aug0.avi\n",
            "Caching 3_dslkaldskjflakjs.mp4\n",
            "Caching 2_sdjfhafsldkjhjk.mp4\n",
            "Caching 3_ewdfkjwaeoihjlkasdjf__aug0.avi\n",
            "Caching 4_kling_20251209_Text_to_Video_Generate_a_190_0__aug1.avi\n",
            "Caching 3_kling_kdjflaskdjf__aug1.avi\n",
            "Caching 4_kling_20251206_Text_to_Video_Generate_a_315_3__aug1.avi\n",
            "Caching 3_kling_kdjflaskdjf__aug0.avi\n",
            "Caching 4_kling_20251209_Image_to_Video_Generate_a_635_1.mp4\n",
            "Caching 3_sdlkfjalkjejafe__aug1.avi\n",
            "Caching 2_kling_20251206_Text_to_Video_Generate_a_71_1__aug0.avi\n",
            "Caching 3_kling_20251206_Text_to_Video_Generate_a_17_0__aug1.avi\n",
            "Caching 3_sdfjwaiejflkasjdf__aug1.avi\n",
            "Caching 3_kling_20251206_Text_to_Video_Generate_a_17_0.mp4\n",
            "Caching 4_pushup_1f2da596-7619-4d55-9376-069e15a42a1a_h264__aug1.avi\n",
            "Caching 3_sdlkfjalkjejafe__aug0.avi\n",
            "Caching 3_dsksdfjbvsdkj__aug1.avi\n",
            "Caching 3_sadklfjasbnlkjlfkj.mp4\n",
            "Caching 5_sadfjhaslfkjasdlkfjsa.mp4\n",
            "Caching 2_sdfkjsaleijflaskdjf__aug1.avi\n",
            "Caching 2_kling_20251205_Text_to_Video_On_a_sandy_4976_0.mp4\n",
            "Cached Posedata in ./pose-data-aug\n"
          ]
        }
      ],
      "source": [
        "cache_all_poses(video_dir=\"./video-data-aug\", pose_dir=\"./pose-data-aug\",\n",
        "                target_frames=300, stride=2, include_visibility=True)\n",
        "print(\"Cached Posedata in ./pose-data-aug\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dJpHlt-fox3Y"
      },
      "source": [
        "## Stitch Videos and Balance"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "blGpPxQoox3Y",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "418d0e73-6870-4bc0-9d37-b059ba2392df"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ Copied 231 base pose files into pose-data-aug-stitched\n",
            "✅ Added self-stitched poses into pose-data-aug-stitched\n",
            "   +8 reps:  63\n",
            "   +9 reps:  102\n",
            "   +10 reps: 6\n",
            "Source pose counts per class:\n",
            "  1: 3\n",
            "  2: 48\n",
            "  3: 102\n",
            "  4: 63\n",
            "  5: 6\n",
            "  6: 6\n",
            "  7: 3\n",
            "  8: 63\n",
            "  9: 102\n",
            "  10: 6\n",
            "\n",
            "✅ Copied 402 original pose files into pose-data-final\n",
            "\n",
            "Counts after copying originals:\n",
            "  1: 3\n",
            "  2: 48\n",
            "  3: 102\n",
            "  4: 63\n",
            "  5: 6\n",
            "  6: 6\n",
            "  7: 3\n",
            "  8: 63\n",
            "  9: 102\n",
            "  10: 6\n",
            "\n",
            "Class 1: need 7 more copies.\n",
            "  -> class 1 now has 10\n",
            "\n",
            "Class 2: already has 48 ≥ 10, skipping.\n",
            "\n",
            "Class 3: already has 102 ≥ 10, skipping.\n",
            "\n",
            "Class 4: already has 63 ≥ 10, skipping.\n",
            "\n",
            "Class 5: need 4 more copies.\n",
            "  -> class 5 now has 10\n",
            "\n",
            "Class 6: need 4 more copies.\n",
            "  -> class 6 now has 10\n",
            "\n",
            "Class 7: need 7 more copies.\n",
            "  -> class 7 now has 10\n",
            "\n",
            "Class 8: already has 63 ≥ 10, skipping.\n",
            "\n",
            "Class 9: already has 102 ≥ 10, skipping.\n",
            "\n",
            "Class 10: need 4 more copies.\n",
            "  -> class 10 now has 10\n",
            "\n",
            "✅ Balanced pose dataset written to: pose-data-final\n"
          ]
        }
      ],
      "source": [
        "# Build Training Pose Directory with Self-Stitched Samples\n",
        "train_pose_dir = build_training_pose_dir_with_selfstitch(\n",
        "    base_pose_dir=\"./pose-data-aug\",\n",
        "    out_pose_dir=\"./pose-data-aug-stitched\",\n",
        "    fade=15,\n",
        "    target_frames=300,\n",
        "    overwrite=True,   # optional; clears old outputs if rerun\n",
        ")\n",
        "\n",
        "# Build Balanced Pose Dataset by Copying\n",
        "balanced_pose_dir = build_pose_dataset_balanced_by_copy(\n",
        "    src_pose_dir=train_pose_dir,\n",
        "    dst_pose_dir=\"./pose-data-final\",\n",
        "    target_per_class=10,\n",
        "    seed=42,\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mf5YanWGox3Y"
      },
      "source": [
        "## Train and Save Model (Val Accuracy is meaningless because we train final model on full dataset --> for actual validation accuracy tune val_split to 0.2 instead of 0.01)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "HN-7rwFKox3Y",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "60b1237e-a47e-4bef-d9f9-a04c91a024ac"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using device: cuda\n",
            "\n",
            "Instantiated model.\n",
            "\n",
            "Train: 229 videos, Val: 2 videos\n",
            "\n",
            "Got dataloaders.\n",
            "\n",
            "Go time. Let the training commence.\n",
            "\n",
            "Validation Accuracy is only a dummy value, since val_split=0.01 and model is trained on entire dataset for maximum performance\n",
            "Epoch 1/200 | Train Loss: 1.8245, Train Acc: 0.3712 | Val Loss: 0.9112, Val Acc: 0.5000, LR: 0.001000\n",
            "Epoch 2/200 | Train Loss: 1.4730, Train Acc: 0.3450 | Val Loss: 1.2812, Val Acc: 0.5000, LR: 0.001000\n",
            "Epoch 3/200 | Train Loss: 1.4011, Train Acc: 0.4192 | Val Loss: 1.1293, Val Acc: 0.5000, LR: 0.001000\n",
            "Epoch 4/200 | Train Loss: 1.4262, Train Acc: 0.4279 | Val Loss: 1.0310, Val Acc: 0.5000, LR: 0.001000\n",
            "Epoch 5/200 | Train Loss: 1.3442, Train Acc: 0.4017 | Val Loss: 1.0332, Val Acc: 0.5000, LR: 0.001000\n",
            "Epoch 6/200 | Train Loss: 1.3266, Train Acc: 0.4192 | Val Loss: 1.0194, Val Acc: 0.5000, LR: 0.001000\n",
            "Epoch 7/200 | Train Loss: 1.3623, Train Acc: 0.4061 | Val Loss: 0.9447, Val Acc: 0.5000, LR: 0.001000\n",
            "Epoch 8/200 | Train Loss: 1.3649, Train Acc: 0.3974 | Val Loss: 0.9798, Val Acc: 0.5000, LR: 0.001000\n",
            "Epoch 9/200 | Train Loss: 1.3088, Train Acc: 0.4279 | Val Loss: 1.0611, Val Acc: 0.5000, LR: 0.001000\n",
            "Epoch 10/200 | Train Loss: 1.3393, Train Acc: 0.4367 | Val Loss: 0.9222, Val Acc: 0.5000, LR: 0.001000\n",
            "Epoch 11/200 | Train Loss: 1.3421, Train Acc: 0.4148 | Val Loss: 1.0209, Val Acc: 1.0000, LR: 0.001000\n",
            "Epoch 12/200 | Train Loss: 1.3591, Train Acc: 0.3624 | Val Loss: 0.8462, Val Acc: 1.0000, LR: 0.001000\n",
            "Epoch 13/200 | Train Loss: 1.3195, Train Acc: 0.3755 | Val Loss: 1.0242, Val Acc: 0.5000, LR: 0.001000\n",
            "Epoch 14/200 | Train Loss: 1.3210, Train Acc: 0.4367 | Val Loss: 0.9686, Val Acc: 0.5000, LR: 0.001000\n",
            "Epoch 15/200 | Train Loss: 1.2839, Train Acc: 0.4323 | Val Loss: 0.9355, Val Acc: 1.0000, LR: 0.001000\n",
            "Epoch 16/200 | Train Loss: 1.3270, Train Acc: 0.4323 | Val Loss: 1.0087, Val Acc: 0.5000, LR: 0.001000\n",
            "Epoch 17/200 | Train Loss: 1.2844, Train Acc: 0.4323 | Val Loss: 0.8981, Val Acc: 1.0000, LR: 0.001000\n",
            "Epoch 18/200 | Train Loss: 1.3566, Train Acc: 0.4410 | Val Loss: 0.9388, Val Acc: 0.5000, LR: 0.001000\n",
            "Epoch 19/200 | Train Loss: 1.2820, Train Acc: 0.4236 | Val Loss: 0.9282, Val Acc: 0.5000, LR: 0.001000\n",
            "Epoch 20/200 | Train Loss: 1.2611, Train Acc: 0.4585 | Val Loss: 0.8700, Val Acc: 0.5000, LR: 0.001000\n",
            "Epoch 21/200 | Train Loss: 1.3108, Train Acc: 0.4541 | Val Loss: 1.3196, Val Acc: 0.0000, LR: 0.001000\n",
            "Epoch 22/200 | Train Loss: 1.2823, Train Acc: 0.4498 | Val Loss: 0.9441, Val Acc: 0.5000, LR: 0.001000\n",
            "Epoch 23/200 | Train Loss: 1.3229, Train Acc: 0.4454 | Val Loss: 0.8975, Val Acc: 0.5000, LR: 0.000900\n",
            "Epoch 24/200 | Train Loss: 1.2754, Train Acc: 0.4323 | Val Loss: 0.9595, Val Acc: 1.0000, LR: 0.000900\n",
            "Epoch 25/200 | Train Loss: 1.3602, Train Acc: 0.3843 | Val Loss: 0.8979, Val Acc: 0.5000, LR: 0.000900\n",
            "Epoch 26/200 | Train Loss: 1.2734, Train Acc: 0.4672 | Val Loss: 0.9221, Val Acc: 0.5000, LR: 0.000900\n",
            "Epoch 27/200 | Train Loss: 1.3667, Train Acc: 0.4498 | Val Loss: 0.9065, Val Acc: 0.5000, LR: 0.000900\n",
            "Epoch 28/200 | Train Loss: 1.2606, Train Acc: 0.4672 | Val Loss: 1.0670, Val Acc: 0.0000, LR: 0.000900\n",
            "Epoch 29/200 | Train Loss: 1.2242, Train Acc: 0.4541 | Val Loss: 0.8611, Val Acc: 0.5000, LR: 0.000900\n",
            "Epoch 30/200 | Train Loss: 1.1761, Train Acc: 0.4760 | Val Loss: 0.9025, Val Acc: 0.5000, LR: 0.000900\n",
            "Epoch 31/200 | Train Loss: 1.1784, Train Acc: 0.4672 | Val Loss: 0.8462, Val Acc: 1.0000, LR: 0.000900\n",
            "Epoch 32/200 | Train Loss: 1.2787, Train Acc: 0.4672 | Val Loss: 0.7393, Val Acc: 1.0000, LR: 0.000900\n",
            "Epoch 33/200 | Train Loss: 1.2063, Train Acc: 0.4760 | Val Loss: 0.9352, Val Acc: 0.5000, LR: 0.000900\n",
            "Epoch 34/200 | Train Loss: 1.1921, Train Acc: 0.4716 | Val Loss: 0.7357, Val Acc: 0.5000, LR: 0.000900\n",
            "Epoch 35/200 | Train Loss: 1.2961, Train Acc: 0.4585 | Val Loss: 0.8053, Val Acc: 1.0000, LR: 0.000900\n",
            "Epoch 36/200 | Train Loss: 1.1887, Train Acc: 0.4847 | Val Loss: 0.8722, Val Acc: 0.5000, LR: 0.000900\n",
            "Epoch 37/200 | Train Loss: 1.1816, Train Acc: 0.4978 | Val Loss: 0.8080, Val Acc: 0.5000, LR: 0.000900\n",
            "Epoch 38/200 | Train Loss: 1.1458, Train Acc: 0.4891 | Val Loss: 1.0491, Val Acc: 0.5000, LR: 0.000900\n",
            "Epoch 39/200 | Train Loss: 1.1691, Train Acc: 0.4847 | Val Loss: 0.7112, Val Acc: 1.0000, LR: 0.000900\n",
            "Epoch 40/200 | Train Loss: 1.1354, Train Acc: 0.5022 | Val Loss: 0.9103, Val Acc: 0.5000, LR: 0.000900\n",
            "Epoch 41/200 | Train Loss: 1.1103, Train Acc: 0.5284 | Val Loss: 0.9800, Val Acc: 0.5000, LR: 0.000900\n",
            "Epoch 42/200 | Train Loss: 1.0962, Train Acc: 0.4847 | Val Loss: 0.7797, Val Acc: 1.0000, LR: 0.000900\n",
            "Epoch 43/200 | Train Loss: 1.0690, Train Acc: 0.5240 | Val Loss: 1.0510, Val Acc: 0.5000, LR: 0.000900\n",
            "Epoch 44/200 | Train Loss: 1.2602, Train Acc: 0.4847 | Val Loss: 0.7825, Val Acc: 1.0000, LR: 0.000900\n",
            "Epoch 45/200 | Train Loss: 1.2012, Train Acc: 0.5153 | Val Loss: 0.9091, Val Acc: 0.5000, LR: 0.000900\n",
            "Epoch 46/200 | Train Loss: 1.1293, Train Acc: 0.4716 | Val Loss: 0.7738, Val Acc: 0.5000, LR: 0.000900\n",
            "Epoch 47/200 | Train Loss: 1.1367, Train Acc: 0.4629 | Val Loss: 0.7982, Val Acc: 1.0000, LR: 0.000900\n",
            "Epoch 48/200 | Train Loss: 1.1032, Train Acc: 0.4978 | Val Loss: 0.8657, Val Acc: 0.5000, LR: 0.000900\n",
            "Epoch 49/200 | Train Loss: 1.1071, Train Acc: 0.4891 | Val Loss: 0.7708, Val Acc: 0.5000, LR: 0.000900\n",
            "Epoch 50/200 | Train Loss: 1.1823, Train Acc: 0.5197 | Val Loss: 0.9270, Val Acc: 0.5000, LR: 0.000810\n",
            "Epoch 51/200 | Train Loss: 1.0486, Train Acc: 0.5328 | Val Loss: 1.0210, Val Acc: 0.5000, LR: 0.000810\n",
            "Epoch 52/200 | Train Loss: 1.0667, Train Acc: 0.5459 | Val Loss: 0.7641, Val Acc: 0.5000, LR: 0.000810\n",
            "Epoch 53/200 | Train Loss: 1.1225, Train Acc: 0.5764 | Val Loss: 0.9873, Val Acc: 1.0000, LR: 0.000810\n",
            "Epoch 54/200 | Train Loss: 1.1240, Train Acc: 0.5066 | Val Loss: 0.9516, Val Acc: 0.5000, LR: 0.000810\n",
            "Epoch 55/200 | Train Loss: 1.1020, Train Acc: 0.5022 | Val Loss: 1.0376, Val Acc: 0.0000, LR: 0.000810\n",
            "Epoch 56/200 | Train Loss: 1.0630, Train Acc: 0.5546 | Val Loss: 0.7695, Val Acc: 1.0000, LR: 0.000810\n",
            "Epoch 57/200 | Train Loss: 1.0765, Train Acc: 0.5371 | Val Loss: 0.7291, Val Acc: 0.5000, LR: 0.000810\n",
            "Epoch 58/200 | Train Loss: 1.0194, Train Acc: 0.5677 | Val Loss: 0.8852, Val Acc: 0.5000, LR: 0.000810\n",
            "Epoch 59/200 | Train Loss: 1.0429, Train Acc: 0.5153 | Val Loss: 1.3213, Val Acc: 0.5000, LR: 0.000810\n",
            "Epoch 60/200 | Train Loss: 1.0558, Train Acc: 0.5197 | Val Loss: 0.8556, Val Acc: 0.5000, LR: 0.000810\n",
            "Epoch 61/200 | Train Loss: 0.9827, Train Acc: 0.5808 | Val Loss: 1.0074, Val Acc: 0.5000, LR: 0.000729\n",
            "Epoch 62/200 | Train Loss: 1.0054, Train Acc: 0.5895 | Val Loss: 1.0149, Val Acc: 0.5000, LR: 0.000729\n",
            "Epoch 63/200 | Train Loss: 1.1157, Train Acc: 0.5371 | Val Loss: 0.8620, Val Acc: 0.5000, LR: 0.000729\n",
            "Epoch 64/200 | Train Loss: 0.9881, Train Acc: 0.5808 | Val Loss: 0.9425, Val Acc: 0.5000, LR: 0.000729\n",
            "Epoch 65/200 | Train Loss: 0.9865, Train Acc: 0.5895 | Val Loss: 1.2259, Val Acc: 0.5000, LR: 0.000729\n",
            "Epoch 66/200 | Train Loss: 0.9547, Train Acc: 0.6157 | Val Loss: 0.9066, Val Acc: 0.5000, LR: 0.000729\n",
            "Epoch 67/200 | Train Loss: 0.9346, Train Acc: 0.6419 | Val Loss: 0.7310, Val Acc: 0.5000, LR: 0.000729\n",
            "Epoch 68/200 | Train Loss: 0.9706, Train Acc: 0.5939 | Val Loss: 0.7455, Val Acc: 0.5000, LR: 0.000729\n",
            "Epoch 69/200 | Train Loss: 0.9878, Train Acc: 0.6026 | Val Loss: 1.3068, Val Acc: 0.5000, LR: 0.000729\n",
            "Epoch 70/200 | Train Loss: 0.8470, Train Acc: 0.6245 | Val Loss: 1.3610, Val Acc: 0.5000, LR: 0.000729\n",
            "Epoch 71/200 | Train Loss: 1.0456, Train Acc: 0.5677 | Val Loss: 0.5833, Val Acc: 1.0000, LR: 0.000729\n",
            "Epoch 72/200 | Train Loss: 1.0126, Train Acc: 0.5983 | Val Loss: 1.1665, Val Acc: 0.0000, LR: 0.000729\n",
            "Epoch 73/200 | Train Loss: 0.9310, Train Acc: 0.5939 | Val Loss: 1.0348, Val Acc: 0.5000, LR: 0.000729\n",
            "Epoch 74/200 | Train Loss: 0.8625, Train Acc: 0.6463 | Val Loss: 1.1474, Val Acc: 0.5000, LR: 0.000729\n",
            "Epoch 75/200 | Train Loss: 0.8652, Train Acc: 0.6507 | Val Loss: 1.1978, Val Acc: 0.5000, LR: 0.000729\n",
            "Epoch 76/200 | Train Loss: 0.9662, Train Acc: 0.6070 | Val Loss: 0.8878, Val Acc: 0.5000, LR: 0.000729\n",
            "Epoch 77/200 | Train Loss: 0.9966, Train Acc: 0.5895 | Val Loss: 1.3794, Val Acc: 0.0000, LR: 0.000729\n",
            "Epoch 78/200 | Train Loss: 1.0510, Train Acc: 0.5633 | Val Loss: 1.6343, Val Acc: 0.5000, LR: 0.000729\n",
            "Epoch 79/200 | Train Loss: 1.0015, Train Acc: 0.5983 | Val Loss: 0.7479, Val Acc: 1.0000, LR: 0.000729\n",
            "Epoch 80/200 | Train Loss: 1.0896, Train Acc: 0.5240 | Val Loss: 1.1908, Val Acc: 0.5000, LR: 0.000729\n",
            "Epoch 81/200 | Train Loss: 1.0281, Train Acc: 0.5939 | Val Loss: 1.0174, Val Acc: 0.5000, LR: 0.000729\n",
            "Epoch 82/200 | Train Loss: 0.9224, Train Acc: 0.5764 | Val Loss: 0.8423, Val Acc: 1.0000, LR: 0.000656\n",
            "Epoch 83/200 | Train Loss: 1.0065, Train Acc: 0.5939 | Val Loss: 0.9473, Val Acc: 0.5000, LR: 0.000656\n",
            "Epoch 84/200 | Train Loss: 0.8426, Train Acc: 0.6376 | Val Loss: 0.8399, Val Acc: 0.5000, LR: 0.000656\n",
            "Epoch 85/200 | Train Loss: 0.9302, Train Acc: 0.6245 | Val Loss: 0.8474, Val Acc: 0.0000, LR: 0.000656\n",
            "Epoch 86/200 | Train Loss: 0.8845, Train Acc: 0.6245 | Val Loss: 0.9076, Val Acc: 0.5000, LR: 0.000656\n",
            "Epoch 87/200 | Train Loss: 0.8992, Train Acc: 0.6288 | Val Loss: 0.8251, Val Acc: 0.5000, LR: 0.000656\n",
            "Epoch 88/200 | Train Loss: 0.8122, Train Acc: 0.6594 | Val Loss: 1.6508, Val Acc: 0.5000, LR: 0.000656\n",
            "Epoch 89/200 | Train Loss: 0.8375, Train Acc: 0.6638 | Val Loss: 0.8122, Val Acc: 0.5000, LR: 0.000656\n",
            "Epoch 90/200 | Train Loss: 0.7785, Train Acc: 0.6550 | Val Loss: 0.8617, Val Acc: 0.5000, LR: 0.000656\n",
            "Epoch 91/200 | Train Loss: 0.7915, Train Acc: 0.6638 | Val Loss: 0.7989, Val Acc: 0.5000, LR: 0.000656\n",
            "Epoch 92/200 | Train Loss: 0.7213, Train Acc: 0.6987 | Val Loss: 0.5491, Val Acc: 1.0000, LR: 0.000656\n",
            "Epoch 93/200 | Train Loss: 0.6978, Train Acc: 0.6900 | Val Loss: 0.9923, Val Acc: 0.5000, LR: 0.000656\n",
            "Epoch 94/200 | Train Loss: 0.7735, Train Acc: 0.6943 | Val Loss: 1.5562, Val Acc: 0.5000, LR: 0.000656\n",
            "Epoch 95/200 | Train Loss: 0.7082, Train Acc: 0.7162 | Val Loss: 1.7385, Val Acc: 0.5000, LR: 0.000656\n",
            "Epoch 96/200 | Train Loss: 0.6905, Train Acc: 0.7336 | Val Loss: 1.7000, Val Acc: 0.5000, LR: 0.000656\n",
            "Epoch 97/200 | Train Loss: 0.6780, Train Acc: 0.6725 | Val Loss: 1.6476, Val Acc: 0.5000, LR: 0.000656\n",
            "Epoch 98/200 | Train Loss: 0.7650, Train Acc: 0.7467 | Val Loss: 2.4296, Val Acc: 0.5000, LR: 0.000656\n",
            "Epoch 99/200 | Train Loss: 0.7901, Train Acc: 0.6638 | Val Loss: 1.6912, Val Acc: 0.5000, LR: 0.000656\n",
            "Epoch 100/200 | Train Loss: 0.6939, Train Acc: 0.7031 | Val Loss: 2.2861, Val Acc: 0.5000, LR: 0.000656\n",
            "Epoch 101/200 | Train Loss: 0.5856, Train Acc: 0.7860 | Val Loss: 1.7970, Val Acc: 0.5000, LR: 0.000656\n",
            "Epoch 102/200 | Train Loss: 0.6617, Train Acc: 0.7511 | Val Loss: 1.9269, Val Acc: 0.5000, LR: 0.000656\n",
            "Epoch 103/200 | Train Loss: 0.6236, Train Acc: 0.7074 | Val Loss: 2.0937, Val Acc: 0.5000, LR: 0.000590\n",
            "Epoch 104/200 | Train Loss: 0.4956, Train Acc: 0.8035 | Val Loss: 1.5717, Val Acc: 0.5000, LR: 0.000590\n",
            "Epoch 105/200 | Train Loss: 0.5250, Train Acc: 0.7598 | Val Loss: 2.3893, Val Acc: 0.5000, LR: 0.000590\n",
            "Epoch 106/200 | Train Loss: 0.4546, Train Acc: 0.8035 | Val Loss: 2.2395, Val Acc: 0.5000, LR: 0.000590\n",
            "Epoch 107/200 | Train Loss: 0.4621, Train Acc: 0.8734 | Val Loss: 1.8464, Val Acc: 0.5000, LR: 0.000590\n",
            "Epoch 108/200 | Train Loss: 0.3810, Train Acc: 0.8646 | Val Loss: 1.9241, Val Acc: 0.5000, LR: 0.000590\n",
            "Epoch 109/200 | Train Loss: 0.3698, Train Acc: 0.8428 | Val Loss: 2.3003, Val Acc: 0.5000, LR: 0.000590\n",
            "Epoch 110/200 | Train Loss: 0.4346, Train Acc: 0.8384 | Val Loss: 2.9197, Val Acc: 0.5000, LR: 0.000590\n",
            "Epoch 111/200 | Train Loss: 0.6656, Train Acc: 0.7380 | Val Loss: 2.3997, Val Acc: 0.5000, LR: 0.000590\n",
            "Epoch 112/200 | Train Loss: 0.5138, Train Acc: 0.7817 | Val Loss: 2.4199, Val Acc: 0.5000, LR: 0.000590\n",
            "Epoch 113/200 | Train Loss: 0.5103, Train Acc: 0.8166 | Val Loss: 2.3128, Val Acc: 0.5000, LR: 0.000590\n",
            "Epoch 114/200 | Train Loss: 0.4428, Train Acc: 0.8515 | Val Loss: 2.4231, Val Acc: 0.5000, LR: 0.000531\n",
            "Epoch 115/200 | Train Loss: 0.4064, Train Acc: 0.8515 | Val Loss: 3.4361, Val Acc: 0.0000, LR: 0.000531\n",
            "Epoch 116/200 | Train Loss: 0.4131, Train Acc: 0.8253 | Val Loss: 3.8488, Val Acc: 0.5000, LR: 0.000531\n",
            "Epoch 117/200 | Train Loss: 0.4301, Train Acc: 0.8122 | Val Loss: 1.9121, Val Acc: 0.5000, LR: 0.000531\n",
            "Epoch 118/200 | Train Loss: 0.3700, Train Acc: 0.8559 | Val Loss: 3.1198, Val Acc: 0.5000, LR: 0.000531\n",
            "Epoch 119/200 | Train Loss: 0.2987, Train Acc: 0.9039 | Val Loss: 3.3146, Val Acc: 0.5000, LR: 0.000531\n",
            "Epoch 120/200 | Train Loss: 0.2737, Train Acc: 0.9039 | Val Loss: 2.6097, Val Acc: 0.5000, LR: 0.000531\n",
            "Epoch 121/200 | Train Loss: 0.2351, Train Acc: 0.9301 | Val Loss: 2.6115, Val Acc: 0.5000, LR: 0.000531\n",
            "Epoch 122/200 | Train Loss: 0.2879, Train Acc: 0.9170 | Val Loss: 4.1437, Val Acc: 0.0000, LR: 0.000531\n",
            "Epoch 123/200 | Train Loss: 0.3879, Train Acc: 0.8384 | Val Loss: 3.0914, Val Acc: 0.5000, LR: 0.000531\n",
            "Epoch 124/200 | Train Loss: 0.3918, Train Acc: 0.8428 | Val Loss: 2.9067, Val Acc: 0.5000, LR: 0.000531\n",
            "Epoch 125/200 | Train Loss: 0.3154, Train Acc: 0.8865 | Val Loss: 4.4664, Val Acc: 0.5000, LR: 0.000478\n",
            "Epoch 126/200 | Train Loss: 0.2163, Train Acc: 0.9476 | Val Loss: 4.1713, Val Acc: 0.5000, LR: 0.000478\n",
            "Epoch 127/200 | Train Loss: 0.2032, Train Acc: 0.9345 | Val Loss: 3.9151, Val Acc: 0.5000, LR: 0.000478\n",
            "Epoch 128/200 | Train Loss: 0.1527, Train Acc: 0.9694 | Val Loss: 2.2727, Val Acc: 0.5000, LR: 0.000478\n",
            "Epoch 129/200 | Train Loss: 0.1150, Train Acc: 0.9651 | Val Loss: 3.8340, Val Acc: 0.5000, LR: 0.000478\n",
            "Epoch 130/200 | Train Loss: 0.1244, Train Acc: 0.9607 | Val Loss: 3.5780, Val Acc: 0.5000, LR: 0.000478\n",
            "Epoch 131/200 | Train Loss: 0.1130, Train Acc: 0.9694 | Val Loss: 2.6380, Val Acc: 0.5000, LR: 0.000478\n",
            "Epoch 132/200 | Train Loss: 0.1230, Train Acc: 0.9607 | Val Loss: 1.3586, Val Acc: 0.5000, LR: 0.000478\n",
            "Epoch 133/200 | Train Loss: 0.1645, Train Acc: 0.9520 | Val Loss: 3.6051, Val Acc: 0.5000, LR: 0.000478\n",
            "Epoch 134/200 | Train Loss: 0.1620, Train Acc: 0.9563 | Val Loss: 3.3860, Val Acc: 0.5000, LR: 0.000478\n",
            "Epoch 135/200 | Train Loss: 0.1876, Train Acc: 0.9345 | Val Loss: 5.6968, Val Acc: 0.5000, LR: 0.000478\n",
            "Epoch 136/200 | Train Loss: 0.2100, Train Acc: 0.9170 | Val Loss: 3.6406, Val Acc: 0.5000, LR: 0.000430\n",
            "Epoch 137/200 | Train Loss: 0.2725, Train Acc: 0.8908 | Val Loss: 0.7874, Val Acc: 0.5000, LR: 0.000430\n",
            "Epoch 138/200 | Train Loss: 0.1493, Train Acc: 0.9607 | Val Loss: 2.4613, Val Acc: 0.5000, LR: 0.000430\n",
            "Epoch 139/200 | Train Loss: 0.1203, Train Acc: 0.9869 | Val Loss: 0.1920, Val Acc: 1.0000, LR: 0.000430\n",
            "Epoch 140/200 | Train Loss: 0.1444, Train Acc: 0.9520 | Val Loss: 3.9618, Val Acc: 0.5000, LR: 0.000430\n",
            "Epoch 141/200 | Train Loss: 0.0906, Train Acc: 0.9869 | Val Loss: 2.5275, Val Acc: 0.5000, LR: 0.000430\n",
            "Epoch 142/200 | Train Loss: 0.1040, Train Acc: 0.9651 | Val Loss: 4.9829, Val Acc: 0.5000, LR: 0.000430\n",
            "Epoch 143/200 | Train Loss: 0.0892, Train Acc: 0.9694 | Val Loss: 1.3644, Val Acc: 0.5000, LR: 0.000430\n",
            "Epoch 144/200 | Train Loss: 0.0623, Train Acc: 0.9825 | Val Loss: 1.5681, Val Acc: 0.5000, LR: 0.000430\n",
            "Epoch 145/200 | Train Loss: 0.0782, Train Acc: 0.9869 | Val Loss: 1.3544, Val Acc: 0.5000, LR: 0.000430\n",
            "Epoch 146/200 | Train Loss: 0.1475, Train Acc: 0.9607 | Val Loss: 3.4674, Val Acc: 0.5000, LR: 0.000430\n",
            "Epoch 147/200 | Train Loss: 0.1427, Train Acc: 0.9563 | Val Loss: 5.2820, Val Acc: 0.5000, LR: 0.000430\n",
            "Epoch 148/200 | Train Loss: 0.1252, Train Acc: 0.9432 | Val Loss: 3.4706, Val Acc: 0.5000, LR: 0.000430\n",
            "Epoch 149/200 | Train Loss: 0.2998, Train Acc: 0.8821 | Val Loss: 0.1797, Val Acc: 1.0000, LR: 0.000430\n",
            "Epoch 150/200 | Train Loss: 0.2225, Train Acc: 0.9170 | Val Loss: 0.5512, Val Acc: 0.5000, LR: 0.000430\n",
            "Epoch 151/200 | Train Loss: 0.1048, Train Acc: 0.9782 | Val Loss: 3.0011, Val Acc: 0.5000, LR: 0.000430\n",
            "Epoch 152/200 | Train Loss: 0.0827, Train Acc: 0.9738 | Val Loss: 3.3826, Val Acc: 0.5000, LR: 0.000430\n",
            "Epoch 153/200 | Train Loss: 0.0721, Train Acc: 0.9825 | Val Loss: 3.5052, Val Acc: 0.5000, LR: 0.000430\n",
            "Epoch 154/200 | Train Loss: 0.0646, Train Acc: 0.9913 | Val Loss: 4.0585, Val Acc: 0.5000, LR: 0.000430\n",
            "Epoch 155/200 | Train Loss: 0.0643, Train Acc: 0.9869 | Val Loss: 3.5529, Val Acc: 0.5000, LR: 0.000430\n",
            "Epoch 156/200 | Train Loss: 0.0493, Train Acc: 0.9913 | Val Loss: 3.8364, Val Acc: 0.5000, LR: 0.000430\n",
            "Epoch 157/200 | Train Loss: 0.0478, Train Acc: 0.9913 | Val Loss: 3.8460, Val Acc: 0.5000, LR: 0.000430\n",
            "Epoch 158/200 | Train Loss: 0.0324, Train Acc: 0.9956 | Val Loss: 4.1392, Val Acc: 0.5000, LR: 0.000430\n",
            "Epoch 159/200 | Train Loss: 0.0833, Train Acc: 0.9869 | Val Loss: 4.4162, Val Acc: 0.5000, LR: 0.000430\n",
            "Epoch 160/200 | Train Loss: 0.0495, Train Acc: 1.0000 | Val Loss: 3.3357, Val Acc: 0.5000, LR: 0.000387\n",
            "Epoch 161/200 | Train Loss: 0.0458, Train Acc: 0.9913 | Val Loss: 4.0463, Val Acc: 0.5000, LR: 0.000387\n",
            "Epoch 162/200 | Train Loss: 0.0593, Train Acc: 0.9913 | Val Loss: 4.3782, Val Acc: 0.5000, LR: 0.000387\n",
            "Epoch 163/200 | Train Loss: 0.0538, Train Acc: 0.9913 | Val Loss: 4.2706, Val Acc: 0.5000, LR: 0.000387\n",
            "Epoch 164/200 | Train Loss: 0.0574, Train Acc: 0.9869 | Val Loss: 5.0569, Val Acc: 0.5000, LR: 0.000387\n",
            "Epoch 165/200 | Train Loss: 0.0291, Train Acc: 1.0000 | Val Loss: 4.9078, Val Acc: 0.5000, LR: 0.000387\n",
            "Epoch 166/200 | Train Loss: 0.0488, Train Acc: 0.9913 | Val Loss: 4.7935, Val Acc: 0.5000, LR: 0.000387\n",
            "Epoch 167/200 | Train Loss: 0.0302, Train Acc: 0.9956 | Val Loss: 4.6285, Val Acc: 0.5000, LR: 0.000387\n",
            "Epoch 168/200 | Train Loss: 0.0271, Train Acc: 1.0000 | Val Loss: 5.0582, Val Acc: 0.5000, LR: 0.000387\n",
            "Epoch 169/200 | Train Loss: 0.0245, Train Acc: 0.9956 | Val Loss: 5.0408, Val Acc: 0.5000, LR: 0.000387\n",
            "Epoch 170/200 | Train Loss: 0.0208, Train Acc: 1.0000 | Val Loss: 5.3256, Val Acc: 0.5000, LR: 0.000387\n",
            "Epoch 171/200 | Train Loss: 0.0177, Train Acc: 0.9956 | Val Loss: 5.2763, Val Acc: 0.5000, LR: 0.000349\n",
            "Epoch 172/200 | Train Loss: 0.0202, Train Acc: 1.0000 | Val Loss: 5.1467, Val Acc: 0.5000, LR: 0.000349\n",
            "Epoch 173/200 | Train Loss: 0.0200, Train Acc: 1.0000 | Val Loss: 5.1953, Val Acc: 0.5000, LR: 0.000349\n",
            "Epoch 174/200 | Train Loss: 0.0119, Train Acc: 1.0000 | Val Loss: 5.2599, Val Acc: 0.5000, LR: 0.000349\n",
            "Epoch 175/200 | Train Loss: 0.0220, Train Acc: 1.0000 | Val Loss: 5.5349, Val Acc: 0.5000, LR: 0.000349\n",
            "Epoch 176/200 | Train Loss: 0.0264, Train Acc: 0.9913 | Val Loss: 5.4033, Val Acc: 0.5000, LR: 0.000349\n",
            "Epoch 177/200 | Train Loss: 0.0153, Train Acc: 1.0000 | Val Loss: 4.7715, Val Acc: 0.5000, LR: 0.000349\n",
            "Epoch 178/200 | Train Loss: 0.0121, Train Acc: 1.0000 | Val Loss: 5.5055, Val Acc: 0.5000, LR: 0.000349\n",
            "Epoch 179/200 | Train Loss: 0.0146, Train Acc: 1.0000 | Val Loss: 5.3383, Val Acc: 0.5000, LR: 0.000349\n",
            "Epoch 180/200 | Train Loss: 0.0119, Train Acc: 1.0000 | Val Loss: 5.3096, Val Acc: 0.5000, LR: 0.000349\n",
            "Epoch 181/200 | Train Loss: 0.0104, Train Acc: 1.0000 | Val Loss: 5.4718, Val Acc: 0.5000, LR: 0.000349\n",
            "Epoch 182/200 | Train Loss: 0.0110, Train Acc: 1.0000 | Val Loss: 5.5712, Val Acc: 0.5000, LR: 0.000314\n",
            "Epoch 183/200 | Train Loss: 0.0093, Train Acc: 1.0000 | Val Loss: 5.4445, Val Acc: 0.5000, LR: 0.000314\n",
            "Epoch 184/200 | Train Loss: 0.0121, Train Acc: 1.0000 | Val Loss: 5.5756, Val Acc: 0.5000, LR: 0.000314\n",
            "Epoch 185/200 | Train Loss: 0.0138, Train Acc: 1.0000 | Val Loss: 5.7035, Val Acc: 0.5000, LR: 0.000314\n",
            "Epoch 186/200 | Train Loss: 0.0175, Train Acc: 0.9956 | Val Loss: 5.3906, Val Acc: 0.5000, LR: 0.000314\n",
            "Epoch 187/200 | Train Loss: 0.0114, Train Acc: 1.0000 | Val Loss: 5.0005, Val Acc: 0.5000, LR: 0.000314\n",
            "Epoch 188/200 | Train Loss: 0.0133, Train Acc: 1.0000 | Val Loss: 5.1573, Val Acc: 0.5000, LR: 0.000314\n",
            "Epoch 189/200 | Train Loss: 0.0114, Train Acc: 1.0000 | Val Loss: 5.6259, Val Acc: 0.5000, LR: 0.000314\n",
            "Epoch 190/200 | Train Loss: 0.0110, Train Acc: 1.0000 | Val Loss: 5.7313, Val Acc: 0.5000, LR: 0.000314\n",
            "Epoch 191/200 | Train Loss: 0.0089, Train Acc: 1.0000 | Val Loss: 5.6860, Val Acc: 0.5000, LR: 0.000314\n",
            "Epoch 192/200 | Train Loss: 0.0091, Train Acc: 1.0000 | Val Loss: 5.7741, Val Acc: 0.5000, LR: 0.000314\n",
            "Epoch 193/200 | Train Loss: 0.0103, Train Acc: 0.9956 | Val Loss: 5.7963, Val Acc: 0.5000, LR: 0.000282\n",
            "Epoch 194/200 | Train Loss: 0.0077, Train Acc: 1.0000 | Val Loss: 5.6543, Val Acc: 0.5000, LR: 0.000282\n",
            "Epoch 195/200 | Train Loss: 0.0079, Train Acc: 1.0000 | Val Loss: 5.6867, Val Acc: 0.5000, LR: 0.000282\n",
            "Epoch 196/200 | Train Loss: 0.0127, Train Acc: 0.9956 | Val Loss: 5.7073, Val Acc: 0.5000, LR: 0.000282\n",
            "Epoch 197/200 | Train Loss: 0.0058, Train Acc: 1.0000 | Val Loss: 5.7703, Val Acc: 0.5000, LR: 0.000282\n",
            "Epoch 198/200 | Train Loss: 0.0074, Train Acc: 1.0000 | Val Loss: 5.7694, Val Acc: 0.5000, LR: 0.000282\n",
            "Epoch 199/200 | Train Loss: 0.0071, Train Acc: 1.0000 | Val Loss: 5.8308, Val Acc: 0.5000, LR: 0.000282\n",
            "Epoch 200/200 | Train Loss: 0.0109, Train Acc: 1.0000 | Val Loss: 6.0938, Val Acc: 0.5000, LR: 0.000282\n",
            "Model saved to mv-final-assignment-gru.pt\n"
          ]
        }
      ],
      "source": [
        "# setting a manual seed for reproducibility\n",
        "torch.manual_seed(0)\n",
        "model = train_model(epochs=200,\n",
        "                    pose_dir=\"./pose-data-aug\")\n",
        "\n",
        "save_model(model, \"mv-final-assignment-gru.pt\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OtxiLVyHox3Y"
      },
      "source": [
        "## Upload to Huggingface (Uncomment this Code in case you want to reupload model)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "6UhvNA5Iox3Y",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 238,
          "referenced_widgets": [
            "1a548a96eee344cca590fdc0ff28890c",
            "f9ea1fa5ca6941c394f8762efd914f8d",
            "c065a8d729834bc89b87358d9624f315",
            "e2b66f3934134920afbbee75623f917a",
            "1040324acd7348339fab85d84aef1bbb",
            "035c7646c555475fb9e9eeb6f0b43f5c",
            "eb965e2808b04eb29909b24401e6428d",
            "a435b8102378432ea1990ae5d4a2a64f",
            "f1119b10eb744af8b2272631873de616",
            "e09d6bb35d8f4814b245c3f34ec887cc",
            "a1ef1d361edb4d85a5956066e34e0b55",
            "b37b9fda83f84d138ca0e9c3ac42d615",
            "f22e2a69e0a64a32ae53cb505efe1587",
            "ce2d33ae8aec453abe44a632389feff3",
            "ff1b66440aa6405891bf287927936c76",
            "aeac717652424503a85b55df10976668",
            "27d97647fd03433db14dc665f5ce4a0c",
            "4ab2cab5aa5e423fb8bd4ad09ff69b02",
            "82229359295747478e51787361067617",
            "115875aea2a049d39cf30aa116411608",
            "a5c4c2493a264637b472fa90049d6efb",
            "51d7309e968e4786a950428dfc7a25b6",
            "580123d6f7884dcea1247db12246a0d3",
            "783632aea5494c2f87d8f74adf47f8ef",
            "5b9a6ca8eb5d438f9792b8f3720c5895",
            "17439f503d73409a8fdb62685769bfae",
            "3587f132e1a84200a75982e8861753d1",
            "acd17790d5734eada03f0f5066c79493",
            "bb4893f33b04498b9d7965e69d9bca00",
            "1515b869ce44442186b8091df6f87689",
            "e7d10892476f47cbafb94f97818ccf3a",
            "91d158a5e628470caf601922fe95004f",
            "8c2fa841b44a4bb9ae99c99222d2b872"
          ]
        },
        "outputId": "fd27be1f-9e2b-41ed-8489-daa160340ec6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Processing Files (0 / 0)      : |          |  0.00B /  0.00B            "
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "1a548a96eee344cca590fdc0ff28890c"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "New Data Upload               : |          |  0.00B /  0.00B            "
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "b37b9fda83f84d138ca0e9c3ac42d615"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "  mv-final-assignment-gru.pt  :  20%|##        |  557kB / 2.73MB            "
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "580123d6f7884dcea1247db12246a0d3"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Uploaded mv-final-assignment-gru.pt to https://huggingface.co/jonny-vr/mv-final-assignment-gru\n"
          ]
        }
      ],
      "source": [
        "# --------------- Upload to Hugging Face Hub --------------\n",
        "#!pip install -q huggingface_hub\n",
        "# from huggingface_hub import login\n",
        "# login(\"MY_HF_API_TOKEN\")  # <-- replace with your token or use environment variable\n",
        "\n",
        "# hf_username = \"jonny-vr\"\n",
        "\n",
        "\n",
        "# def upload_to_hf(\n",
        "#     local_path=\"mv-final-assignment-gru.pt\",\n",
        "#     repo_id=\"jonny-vr/mv-final-assignment-gru-notebook\",   # <-- change if you want\n",
        "#     filename_in_repo=\"mv-final-assignment-gru.pt\",\n",
        "#     commit_message=\"Upload trained GRU pose model\",\n",
        "#     private=False\n",
        "# ):\n",
        "#     api = HfApi()\n",
        "\n",
        "#     # Create repo if it doesn't exist\n",
        "#     api.create_repo(repo_id=repo_id, repo_type=\"model\",\n",
        "#                     exist_ok=True, private=private)\n",
        "\n",
        "#     # Upload the file\n",
        "#     api.upload_file(\n",
        "#         path_or_fileobj=local_path,\n",
        "#         path_in_repo=filename_in_repo,\n",
        "#         repo_id=repo_id,\n",
        "#         repo_type=\"model\",\n",
        "#         commit_message=commit_message,\n",
        "#     )\n",
        "#     print(f\"Uploaded {local_path} to https://huggingface.co/{repo_id}\")\n",
        "\n",
        "\n",
        "# upload_to_hf(\n",
        "#     local_path=\"mv-final-assignment-gru.pt\",\n",
        "#     repo_id=f\"{hf_username}/mv-final-assignment-gru\",\n",
        "# )"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "L4",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "1a548a96eee344cca590fdc0ff28890c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_f9ea1fa5ca6941c394f8762efd914f8d",
              "IPY_MODEL_c065a8d729834bc89b87358d9624f315",
              "IPY_MODEL_e2b66f3934134920afbbee75623f917a"
            ],
            "layout": "IPY_MODEL_1040324acd7348339fab85d84aef1bbb"
          }
        },
        "f9ea1fa5ca6941c394f8762efd914f8d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_035c7646c555475fb9e9eeb6f0b43f5c",
            "placeholder": "​",
            "style": "IPY_MODEL_eb965e2808b04eb29909b24401e6428d",
            "value": "Processing Files (1 / 1)      : 100%"
          }
        },
        "c065a8d729834bc89b87358d9624f315": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_a435b8102378432ea1990ae5d4a2a64f",
            "max": 1,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_f1119b10eb744af8b2272631873de616",
            "value": 1
          }
        },
        "e2b66f3934134920afbbee75623f917a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_e09d6bb35d8f4814b245c3f34ec887cc",
            "placeholder": "​",
            "style": "IPY_MODEL_a1ef1d361edb4d85a5956066e34e0b55",
            "value": " 2.73MB / 2.73MB,  911kB/s  "
          }
        },
        "1040324acd7348339fab85d84aef1bbb": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "035c7646c555475fb9e9eeb6f0b43f5c": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "eb965e2808b04eb29909b24401e6428d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "a435b8102378432ea1990ae5d4a2a64f": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": "20px"
          }
        },
        "f1119b10eb744af8b2272631873de616": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "e09d6bb35d8f4814b245c3f34ec887cc": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "a1ef1d361edb4d85a5956066e34e0b55": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "b37b9fda83f84d138ca0e9c3ac42d615": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_f22e2a69e0a64a32ae53cb505efe1587",
              "IPY_MODEL_ce2d33ae8aec453abe44a632389feff3",
              "IPY_MODEL_ff1b66440aa6405891bf287927936c76"
            ],
            "layout": "IPY_MODEL_aeac717652424503a85b55df10976668"
          }
        },
        "f22e2a69e0a64a32ae53cb505efe1587": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_27d97647fd03433db14dc665f5ce4a0c",
            "placeholder": "​",
            "style": "IPY_MODEL_4ab2cab5aa5e423fb8bd4ad09ff69b02",
            "value": "New Data Upload               : 100%"
          }
        },
        "ce2d33ae8aec453abe44a632389feff3": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_82229359295747478e51787361067617",
            "max": 1,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_115875aea2a049d39cf30aa116411608",
            "value": 1
          }
        },
        "ff1b66440aa6405891bf287927936c76": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_a5c4c2493a264637b472fa90049d6efb",
            "placeholder": "​",
            "style": "IPY_MODEL_51d7309e968e4786a950428dfc7a25b6",
            "value": " 2.73MB / 2.73MB,  911kB/s  "
          }
        },
        "aeac717652424503a85b55df10976668": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "27d97647fd03433db14dc665f5ce4a0c": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "4ab2cab5aa5e423fb8bd4ad09ff69b02": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "82229359295747478e51787361067617": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": "20px"
          }
        },
        "115875aea2a049d39cf30aa116411608": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "a5c4c2493a264637b472fa90049d6efb": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "51d7309e968e4786a950428dfc7a25b6": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "580123d6f7884dcea1247db12246a0d3": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_783632aea5494c2f87d8f74adf47f8ef",
              "IPY_MODEL_5b9a6ca8eb5d438f9792b8f3720c5895",
              "IPY_MODEL_17439f503d73409a8fdb62685769bfae"
            ],
            "layout": "IPY_MODEL_3587f132e1a84200a75982e8861753d1"
          }
        },
        "783632aea5494c2f87d8f74adf47f8ef": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_acd17790d5734eada03f0f5066c79493",
            "placeholder": "​",
            "style": "IPY_MODEL_bb4893f33b04498b9d7965e69d9bca00",
            "value": "  mv-final-assignment-gru.pt  : 100%"
          }
        },
        "5b9a6ca8eb5d438f9792b8f3720c5895": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_1515b869ce44442186b8091df6f87689",
            "max": 2733769,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_e7d10892476f47cbafb94f97818ccf3a",
            "value": 2733769
          }
        },
        "17439f503d73409a8fdb62685769bfae": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_91d158a5e628470caf601922fe95004f",
            "placeholder": "​",
            "style": "IPY_MODEL_8c2fa841b44a4bb9ae99c99222d2b872",
            "value": " 2.73MB / 2.73MB            "
          }
        },
        "3587f132e1a84200a75982e8861753d1": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "acd17790d5734eada03f0f5066c79493": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "bb4893f33b04498b9d7965e69d9bca00": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "1515b869ce44442186b8091df6f87689": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "e7d10892476f47cbafb94f97818ccf3a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "91d158a5e628470caf601922fe95004f": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "8c2fa841b44a4bb9ae99c99222d2b872": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}